{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe1 \ud83e\uddea Home Lab","text":""},{"location":"#overview","title":"\ud83d\udcd6 Overview","text":"<p>This mono repository houses the infrastructure for my homelab. I try to adhere to Infrastructure as Code (IaC) and GitOps practices using tools like Ansible, Terraform, Kubernetes, Flux, Renovate and GitHub Actions.</p>"},{"location":"#how-to-get-started","title":"\u26f5 How to get started","text":"<p>There is a template over at onedr0p/flux-cluster-template if you want to try and follow along with some of the practices I use here.</p>"},{"location":"#cluster-components","title":"\ud83c\udfa8 Cluster components","text":"<ul> <li>actions-runner-controller: Self-hosted Github runners</li> <li>cert-manager: Creates SSL certificates for services in my k3s cluster.</li> <li>cilium: Internal Kubernetes networking plugin.</li> <li>external-dns: Automatically manages DNS records from my cluster in a cloud DNS provider.</li> <li>external-secrets: Managed Kubernetes secrets using Doppler.</li> <li>ingress-nginx: Ingress controller to expose HTTP traffic to pods over DNS.</li> <li>Rook: Distributed block storage for persistent storage.</li> <li>sops: Managed secrets for Kubernetes, Ansible and Terraform which are commited to Git.</li> <li>volsync and snapscheduler: Backup and recovery of persistent volume claims.</li> </ul> <p>... and more!</p>"},{"location":"#hardware","title":"\ud83d\uddc4\ufe0f Hardware","text":"## Production Nodes  | Device             | Count | Specs                                                                                                                                                                                                                                                                                                          | OS                                                  | Purpose | |--------------------|-------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|---------| | MinisForum um350   | 1     | **RAM** `32GB`  **M.2 SSD** `500GB`  **HDD SSD** `500GB`                                                                                                                                                       | Ubuntu 22.04.4 LTS                     |  Control Plane       | | MinisForum um350   | 1     | **RAM** `32GB`  **M.2 SSD** `500GB`  **HDD SSD** `500GB`                                                                                                                                                     | Ubuntu 22.04.4 LTS                       |  Data Plane       | | Minisforum um560   | 1     | **RAM** `32GB`   **M.2 SSD** `1TB`  **HDD SSD** `500GB`                                                                                                                                                    | Ubuntu 22.04.4 LTS                     |    Data Plane     |  ## Staging Nodes  | Device             | Count | Specs                                                                                                                                                                                                                                                                                                          | OS                                                  | Purpose | |--------------------|-------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|---------| | Turing Pi RK1      | 3     | **RAM** `16GB`  **M.2 SSD** `500GB`| Ubuntu 22.04 LTS |   Development &amp; Staging      |  ## Infrastructure  | Device             | Count | Specs                                                                                                                                                                                                                                                                                                         | OS       | Purpose | |--------------------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|---------| | Turing Pi RK1      | 1    | **RAM** `16GB`  **M.2 SSD** `500GB`  **HDD** `1TB` | -  | DNS Server | | Turing Pi Board V2 | 1     | -   |   -       |     -    | | Unifi UDM Pro      | 1     | - | - |    10Gb Core Switch + Router + FW    | | Unifi Lite 8 PoE     | 1     | - | - |    Switch    | | Mac Mini     | 1     | **RAM** `8GB`  **M.2 SSD** `256GB` | - |    TBD    |"},{"location":"#changelog","title":"\ud83d\udcdc Changelog","text":"<p>See my awful commit main history and legacy history</p>"},{"location":"#gratitude-and-thanks","title":"Gratitude and thanks","text":"<p>Thanks all the people of Home Operations Discord community who put a lot of effort and donate their time to the community.</p>"},{"location":"#license","title":"\ud83d\udd0f License","text":"<p>See LICENSE v.g WTF License</p>"},{"location":"00-getting-started/about/","title":"About","text":"<p>This documentation uses the Di\u00e1taxis technical documentation system, so is split between four modes of documentation:</p> <ul> <li>Getting started</li> <li>How to Guides</li> <li>Knowledge Base</li> <li>Reference</li> </ul> <p>The website is generated using Material for MkDocs and can be viewed at homelab.oscaromeu.io</p> <p>If you have any questions, don't hesitate to ask me on the community's Discord server.</p>"},{"location":"00-getting-started/about/#key-points","title":"Key Points","text":"<ul> <li>The essentials of setting up and running the laboratory with ease.</li> <li>How to operate the laboratory. Note that this document won't delve into every use case or technology in-depth.</li> <li>Architecture diagrams and the vital roles of each component.</li> <li>Observability.</li> </ul>"},{"location":"01-how-to-guides/","title":"How-to's guides","text":"<p>This site is under construction. Stay tuned for more updates.</p>"},{"location":"02-docs/","title":"Index","text":""},{"location":"02-docs/#this-site-is-under-construction-stay-tuned-for-more-updates","title":"This site is under construction. Stay tuned for more updates.","text":""},{"location":"03-tools/","title":"Troubleshooting","text":"<p>This site is under construction. Stay tuned for more updates.</p>"},{"location":"03-tools/elastic/","title":"Index","text":"<p>Elasticsearch cheatsheet</p>"},{"location":"03-tools/elastic/#shortlinks","title":"Shortlinks:","text":"<ul> <li>Cluster Health</li> <li>Index Level</li> <li>Shard Level</li> <li>Nodes Overview</li> <li>Indices Overview</li> <li>Cluster Maintenance</li> <li>Settings</li> <li>Cluster Settings</li> <li>Ingest</li> <li>Mapping</li> <li>Check Fields in Mappings</li> <li>Close API</li> <li>Search</li> <li>Using the Search API</li> <li>Query</li> <li>Query by Match</li> <li>Query with Bool</li> <li>Other Examples with Query</li> <li>Sort</li> <li>Aggregate</li> <li>Delete</li> <li>Snapshots</li> <li>Create Snapshot Repository on S3</li> <li>Create a Snapshot</li> <li>Restore from a Snapshot</li> </ul>"},{"location":"03-tools/elastic/#resources","title":"Resources","text":"<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html</li> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html</li> <li>https://www.elastic.co/blog/managing-time-based-indices-efficiently</li> <li>http://joelabrahamsson.com/elasticsearch-101/</li> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html</li> <li>https://chatbots.network/logstash-exclude-bots-from-result/</li> </ul>"},{"location":"03-tools/elastic/#overview","title":"Overview","text":""},{"location":"03-tools/elastic/#cluster-health","title":"Cluster Health:","text":"<p>Resource: - https://www.elastic.co/guide/en/elasticsearch/guide/current/_cluster_health.html</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cluster/health?pretty\n{\n  \"cluster_name\" : \"docker-cluster\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 5,\n  \"number_of_data_nodes\" : 5,\n  \"active_primary_shards\" : 11,\n  \"active_shards\" : 22,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 100.0\n}\n</code></pre>"},{"location":"03-tools/elastic/#cluster-health-index-level","title":"Cluster Health: Index Level:","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cluster/health?level=indices&amp;pretty'\n{\n  \"cluster_name\" : \"swarm-elasticsearch\",\n  \"status\" : \"red\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 5,\n  \"number_of_data_nodes\" : 5,\n  \"active_primary_shards\" : 44,\n  \"active_shards\" : 44,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 64,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 40.74074074074074,\n  \"indices\" : {\n    \"test\" : {\n      \"status\" : \"yellow\",\n      \"number_of_shards\" : 5,\n      \"number_of_replicas\" : 1,\n      \"active_primary_shards\" : 5,\n      \"active_shards\" : 5,\n      \"relocating_shards\" : 0,\n      \"initializing_shards\" : 0,\n      \"unassigned_shards\" : 5\n    }\n  }\n}\n</code></pre>"},{"location":"03-tools/elastic/#cluster-health-shard-level","title":"Cluster Health: Shard Level:","text":"<pre><code>curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cluster/health?level=shards&amp;pretty'\n{\n  \"cluster_name\" : \"swarm-elasticsearch\",\n  \"status\" : \"red\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 5,\n  \"number_of_data_nodes\" : 5,\n  \"active_primary_shards\" : 44,\n  \"active_shards\" : 44,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 64,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 40.74074074074074,\n  \"indices\" : {\n    \"test\" : {\n      \"status\" : \"yellow\",\n      \"number_of_shards\" : 5,\n      \"number_of_replicas\" : 1,\n      \"active_primary_shards\" : 5,\n      \"active_shards\" : 5,\n      \"relocating_shards\" : 0,\n      \"initializing_shards\" : 0,\n      \"unassigned_shards\" : 5,\n      \"shards\" : {\n        \"0\" : {\n          \"status\" : \"yellow\",\n          \"primary_active\" : true,\n          \"active_shards\" : 1,\n          \"relocating_shards\" : 0,\n          \"initializing_shards\" : 0,\n          \"unassigned_shards\" : 1\n        },\n        \"1\" : {\n          \"status\" : \"yellow\",\n          \"primary_active\" : true,\n          \"active_shards\" : 1,\n          \"relocating_shards\" : 0,\n          \"initializing_shards\" : 0,\n          \"unassigned_shards\" : 1\n        },\n        \"2\" : {\n          \"status\" : \"yellow\",\n          \"primary_active\" : true,\n          \"active_shards\" : 1,\n          \"relocating_shards\" : 0,\n          \"initializing_shards\" : 0,\n          \"unassigned_shards\" : 1\n        },\n        \"3\" : {\n          \"status\" : \"yellow\",\n          \"primary_active\" : true,\n          \"active_shards\" : 1,\n          \"relocating_shards\" : 0,\n          \"initializing_shards\" : 0,\n          \"unassigned_shards\" : 1\n        },\n        \"4\" : {\n          \"status\" : \"yellow\",\n          \"primary_active\" : true,\n          \"active_shards\" : 1,\n          \"relocating_shards\" : 0,\n          \"initializing_shards\" : 0,\n          \"unassigned_shards\" : 1\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"03-tools/elastic/#nodes-overview","title":"Nodes Overview:","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/nodes?v\nip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\n10.0.2.28           21          92   2    0.55    0.45     0.38 mdi       -      ea1q921\n10.0.2.24           27          95   5    0.17    0.24     0.22 mdi       -      rNDYCtL\n10.0.2.27           20          93  12    0.18    0.20     0.24 mdi       -      bDWFHuw\n10.0.2.18           12          93  12    0.18    0.20     0.24 mdi       *      mstWlao\n10.0.2.22           27          92   2    0.55    0.45     0.38 mdi       -      ifgr6ym\n</code></pre>"},{"location":"03-tools/elastic/#who-is-master","title":"Who is Master:","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/master?v\nid                     host      ip        node\nmstWlaoyTM69xhSt-_rZAA 10.0.2.18 10.0.2.18 mstWlao\n</code></pre>"},{"location":"03-tools/elastic/#indices-overview","title":"Indices Overview:","text":"<p>View all your indices in your cluster:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/indices?v\nhealth status index                         uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   ruan-test                     CrQZB2L4SaaYCkvYPx5vUA   5   1         38            0    131.9kb         78.6kb\n</code></pre> <p>View one index:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/_cat/indices/index-name-2018.01.01?v'\nhealth status index                   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   index-name-2018.01.01 Nk8SMQvRSIaNm854bc3Zjg   5   1     395552            0    755.6mb        377.8mb\n</code></pre> <p>View a range of indices:</p> <pre><code>$ curl -XGET 'https://http://127.0.0.1:9200/_cat/indices/index-name-2018.01*?v'\nhealth status index                   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   index-name-2018.01.19 Vp1EBoeMQkS-a_upLzedhQ   5   1       1220            0      2.6mb          1.3mb\ngreen  open   index-name-2018.01.17 hSJMzFJIQrePifCfgb1rOA   5   1       2875            0      3.8mb          1.9mb\n</code></pre> <p>View only the index name header:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/_cat/indices/*2018.03.*?v&amp;h=index'\nindex\nindex-name-2018.03.01\nindex-name-2018.03.02\n</code></pre>"},{"location":"03-tools/elastic/#how-many-documents-in-the-es-cluster-across-all-indices","title":"How Many Documents in the ES Cluster (Across all Indices):","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/count?v\nepoch      timestamp count\n1502288579 14:22:59  38\n</code></pre>"},{"location":"03-tools/elastic/#shards-info-per-index","title":"Shards Info per Index:","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/shards/ruan-test?v\nindex     shard prirep state   docs  store ip        node\nruan-test 3     r      STARTED   10  6.9kb 10.0.2.28 ea1q921\nruan-test 3     p      STARTED   10  6.9kb 10.0.2.24 rNDYCtL\nruan-test 1     r      STARTED    9 22.7kb 10.0.2.28 ea1q921\nruan-test 1     p      STARTED    9 22.7kb 10.0.2.18 mstWlao\nruan-test 4     r      STARTED    3  6.6kb 10.0.2.22 ifgr6ym\nruan-test 4     p      STARTED    3  6.6kb 10.0.2.18 mstWlao\nruan-test 2     p      STARTED   12 29.2kb 10.0.2.27 bDWFHuw\nruan-test 2     r      STARTED   12  3.9kb 10.0.2.24 rNDYCtL\nruan-test 0     p      STARTED    4 12.9kb 10.0.2.22 ifgr6ym\nruan-test 0     r      STARTED    4 12.9kb 10.0.2.27 bDWFHuw\n</code></pre>"},{"location":"03-tools/elastic/#shard-allocation-per-node","title":"Shard Allocation per Node:","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/allocation?v\nshards disk.indices disk.used disk.avail disk.total disk.percent host      ip        node\n     4       60.6mb    15.7gb     29.9gb     45.7gb           34 10.0.2.24 10.0.2.24 rNDYCtL\n     4       48.3kb    16.7gb     28.9gb     45.7gb           36 10.0.2.18 10.0.2.18 mstWlao\n     4      248.8kb    15.5gb     30.1gb     45.7gb           34 10.0.2.28 10.0.2.28 ea1q921\n     5       54.6mb    16.7gb     28.9gb     45.7gb           36 10.0.2.27 10.0.2.27 bDWFHuw\n     5        3.1mb    15.5gb     30.1gb     45.7gb           34 10.0.2.22 10.0.2.22 ifgr6ym\n</code></pre>"},{"location":"03-tools/elastic/#cluster-maintenance","title":"Cluster Maintenance:","text":""},{"location":"03-tools/elastic/#decomission-node-from-shard-allocation","title":"Decomission Node from Shard Allocation","text":"<p>This will move shards from the mentioned node</p> <pre><code>$ curl -XPUT 'localhost:9200/_cluster/settings?pretty' -d'\n{\n  \"transient\" : {\n    \"cluster.routing.allocation.exclude._ip\" : \"10.0.0.1\"\n  }\n}\n'\n</code></pre>"},{"location":"03-tools/elastic/#recovery-resources","title":"Recovery Resources:","text":"<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-recovery.html</li> </ul>"},{"location":"03-tools/elastic/#recovering-from-node-failure","title":"Recovering from Node Failure:","text":"<p>At the moment one of the nodes were down, and up again:</p> <pre><code>$ curl -XGET http://127.0.0.1:9200/_cat/allocation?v\nshards disk.indices disk.used disk.avail disk.total disk.percent host        ip          node\n   290       54.1mb       1gb       20mb        1gb           98 10.79.2.193 10.79.2.193 es01\n   151       43.5mb       1gb     11.9gb       13gb            8 10.79.3.171 10.79.3.171 es02\n   139                                                                                   UNASSIGNED\n</code></pre>"},{"location":"03-tools/elastic/#recovery-api","title":"Recovery API:","text":"<pre><code>$ curl -XGET http://127.0.0.1:9200/_cat/recovery?v\nindex                     shard time   type       stage source_host target_host repository snapshot files files_percent bytes  bytes_percent total_files total_bytes translog translog_percent total_translog\nsysadmins-2017.06.19      0     1512   replica    done  10.79.2.193 10.79.3.171 n/a        n/a      31    100.0%        340020 100.0%        31          340020      0        100.0%           0\nsysadmins-2017.06.19      0     7739   store      done  10.79.2.193 10.79.2.193 n/a        n/a      0     100.0%        0      100.0%        31          340020      0        100.0%           0\nsysadmins-2017.06.19      1     2592   relocation done  10.79.2.193 10.79.3.171 n/a        n/a      13    100.0%        246229 100.0%        13          246229      0        100.0%           0\nsysadmins-2017.06.19      1     613    replica    done  10.79.3.171 10.79.2.193 n/a        n/a      0     0.0%          0      0.0%          0           0           0        100.0%           0\n</code></pre>"},{"location":"03-tools/elastic/#pending-tasks","title":"Pending Tasks:","text":"<pre><code>$ curl -XGET http://127.0.0.1:9200/_cat/pending_tasks?v\ninsertOrder timeInQueue priority source\n       1736        1.8s URGENT   shard-started ([sysadmins-2017.06.02][2], node[WR3y31g1TnuufpNyrJnQtg], [R], v[91], s[INITIALIZING], a[id=wVTDn4nFSKKxvi07cU0uCg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-08-11T07:50:56.550Z]]), reason [after recovery (replica) from node [{es01}{6ND8sZ_rTqaL42VdlxyW7Q}{10.79.2.193}{10.79.2.193:9300}]]\n       1737        1.3s URGENT   shard-started ([sysadmins-2017.06.02][3], node[WR3y31g1TnuufpNyrJnQtg], [R], v[91], s[INITIALIZING], a[id=JmrtwtYURMyQF6LspeJXLg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-08-11T07:50:56.550Z]]), reason [after recovery (replica) from node [{es01}{6ND8sZ_rTqaL42VdlxyW7Q}{10.79.2.193}{10.79.2.193:9300}]]\n</code></pre>"},{"location":"03-tools/elastic/#clear-cache","title":"Clear Cache:","text":"<pre><code>$  curl -XGET http://127.0.0.1:9200/_cache/clear\n{\"_shards\":{\"total\":21,\"successful\":15,\"failed\":0}}\n</code></pre>"},{"location":"03-tools/elastic/#settings","title":"Settings","text":""},{"location":"03-tools/elastic/#cluster-settings","title":"Cluster Settings","text":"<p>Search Timeout:</p> <p>Global Search Timeout, that applies to all search queries across the entire cluster -&gt; search.default_search_timeout:</p> <pre><code>PUT /_cluster/settings\n{\n    \"persistent\" : {\n        \"search.default_search_timeout\" : \"50\"\n    }\n}\n</code></pre>"},{"location":"03-tools/elastic/#index-info-shards-replicas-allocation","title":"Index Info (Shards, Replicas, Allocation):","text":""},{"location":"03-tools/elastic/#create-index","title":"Create Index:","text":"<p>When you create an Index, 5 Primary Shards and 1 Replica Shard will assigned to the Index by Default.</p> <pre><code>$ curl -XPUT http://elasticsearch:9200/my2ndindex\n{\"acknowledged\":true,\"shards_acknowledged\":true}\n</code></pre> <p>To verify the behavior:</p> <pre><code>curl -XGET -u http://elasticsearch:9200/_cat/indices?v\nhealth status index                         uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   my2ndindex                    V32G9IOoTF6uq0DuNUIAMA   5   1          0            0      1.2kb           650b\ngreen  open   ruan-test                     CrQZB2L4SaaYCkvYPx5vUA   5   1         38            0    131.9kb         78.6kb\n</code></pre> <p>From here on, we can increase the number of replica shards, but NOT the primary shards. You can ONLY set the number primary shards on index creation.</p>"},{"location":"03-tools/elastic/#shard-info-on-our-new-index","title":"Shard info on our new index:","text":"<p>While having 5 prmary shards and 1 replica shard, let's have a look at it:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/shards/my2ndindex?v\nindex      shard prirep state   docs store ip        node\nmy2ndindex 3     p      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 3     r      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 1     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 1     p      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 4     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 4     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 2     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 2     p      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 0     p      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 0     r      STARTED    0  130b 10.0.2.24 rNDYCtL\n</code></pre>"},{"location":"03-tools/elastic/#increase-the-replica-shard-number","title":"Increase the Replica Shard Number:","text":"<p>Let's change the replica shard number to 2, meaning each primary shard will have 2 replica shards:</p> <pre><code>$ curl -XPUT http://elasticsearch:9200/my2ndindex/_settings -d '{\"settings\": {\"index\": {\"number_of_replicas\": 2}}}'\n{\"acknowledged\":true}\n</code></pre> <p>Let's have a look at the shard info after we have increased the replica shard number:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/shards/my2ndindex?v\nindex      shard prirep state   docs store ip        node\nmy2ndindex 3     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 3     p      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 3     r      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 2     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 2     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 2     p      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 4     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 4     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 4     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 1     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 1     p      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 1     r      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 0     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 0     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 0     r      STARTED    0  130b 10.0.2.24 rNDYCtL\n</code></pre>"},{"location":"03-tools/elastic/#create-a-index","title":"Create a Index:","text":"<p>Create a Index with Default Settings:</p> <pre><code>$ curl -XPUT -H 'Content-Type: application/json' 'http://127.0.0.1:9200/ruan-test-2018.03.12'\n</code></pre> <p>View the settings of the created index:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/ruan-test-2018.03.12/_settings?pretty'\n{\n  \"ruan-test-2018.03.12\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1520929659349\",\n        \"number_of_shards\" : \"5\",\n        \"number_of_replicas\" : \"1\",\n        \"uuid\" : \"EwGz6y7XQkK0ZI08u8qdrQ\",\n        \"version\" : {\n          \"created\" : \"6000199\"\n        },\n        \"provided_name\" : \"ruan-test-2018.03.12\"\n      }\n    }\n  }\n}\n</code></pre> <p>Remember that primary shard number can only be set on index creation. Change the settings of the index, let's update the index to: 2 replica shards, and the total_fields limit to: 2000</p> <pre><code>$ curl -XPUT -H 'Content-Type: application/json' 'http://127.0.0.1:9200/ruan-test-2018.03.12/_settings' -d '{\"number_of_replicas\": 0, \"index.mapping.total_fields.limit\": 2000}'\n</code></pre> <p>View the changes:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/ruan-test-2018.03.12/_settings?pretty'\n{\n  \"ruan-test-2018.03.12\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"mapping\" : {\n          \"total_fields\" : {\n            \"limit\" : \"2000\"\n          }\n        },\n        \"number_of_shards\" : \"5\",\n        \"provided_name\" : \"ruan-test-2018.03.12\",\n        \"creation_date\" : \"1520929659349\",\n        \"number_of_replicas\" : \"0\",\n        \"uuid\" : \"EwGz6y7XQkK0ZI08u8qdrQ\",\n        \"version\" : {\n          \"created\" : \"6000199\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Now, to set the settings on Index Creation:</p> <pre><code>$ curl -XPUT -H 'Content-Type: application/json' 'http://127.0.0.1:9200/ruan-test-2018.03.13' -d '{\"settings\": {\"number_of_replicas\": 1, \"number_of_shards\": 2, \"index.mapping.total_fields.limit\": 2000}}'\n</code></pre> <p>Verifying our settings:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/ruan-test-2018.03.13/_settings?pretty'\n{\n  \"ruan-test-2018.03.13\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"mapping\" : {\n          \"total_fields\" : {\n            \"limit\" : \"2000\"\n          }\n        },\n        \"number_of_shards\" : \"2\",\n        \"provided_name\" : \"ruan-test-2018.03.13\",\n        \"creation_date\" : \"1520929638792\",\n        \"number_of_replicas\" : \"1\",\n        \"uuid\" : \"hEY8HrlRTFuiYLwKVDAraQ\",\n        \"version\" : {\n          \"created\" : \"6000199\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Viewing our indexes:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/_cat/indices/ruan-test-*?v'\nhealth status index                uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   ruan-test-2018.03.12 EwGz6y7XQkK0ZI08u8qdrQ   5   1          2            0     15.7kb          7.8kb\ngreen  open   ruan-test-2018.03.13 hEY8HrlRTFuiYLwKVDAraQ   2   1          0            0       932b           466b\n</code></pre>"},{"location":"03-tools/elastic/#ingest-document-into-elasticsearch","title":"Ingest Document into Elasticsearch:","text":"<p>Let's ingest one docuemnt into Elasticsearch, and in this case we will specify the document id as <code>1</code></p> <pre><code>$ curl -XPUT http://elasticsearch:9200/my2ndindex/docs/1 -d '{\"identity\": {\"name\": \"ruan\", \"surname\": \"bekker\"}}'\n{\"_index\":\"my2ndindex\",\"_type\":\"docs\",\"_id\":\"1\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":3,\"successful\":3,\"failed\":0},\"created\":true}\n</code></pre> <p>View the index info:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/indices/my*?v'\nhealth status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   my2ndindex V32G9IOoTF6uq0DuNUIAMA   5   2          1            0       13kb          4.3kb\n</code></pre> <p>View the Shard information on our Index:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/shards/my2ndindex?v\nindex      shard prirep state   docs store ip        node\nmy2ndindex 3     r      STARTED    1 3.9kb 10.0.2.28 ea1q921\nmy2ndindex 3     p      STARTED    1 3.9kb 10.0.2.22 ifgr6ym\nmy2ndindex 3     r      STARTED    1 3.9kb 10.0.2.27 bDWFHuw\nmy2ndindex 1     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 1     p      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 1     r      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 4     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 4     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 4     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 2     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy2ndindex 2     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 2     p      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 0     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy2ndindex 0     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 0     r      STARTED    0  130b 10.0.2.24 rNDYCtL\n</code></pre>"},{"location":"03-tools/elastic/#some-info-on-yellow-status","title":"Some info on Yellow Status:","text":"<p>In elasticsearch, a replica shard of its primary shard, will never appear on the same node as the other shards.</p> <p>As we have 5 nodes in our cluster, meaning if we create 5 replica shards, our index will consist of 5 primary shards, each primary shard having 5 replica shards, as a result in a yellow status es cluster.</p> <p>The reasoning for this is that if we take <code>primary shard id 0</code>:</p> <ul> <li>primary shard   - node 1</li> <li>replica shard 1 - node 2</li> <li>replica shard 2 - node 3</li> <li>replica shard 3 - node 4</li> <li>replica shard 4 - node 5</li> <li>replica shard 5 - UNASSIGNED</li> </ul> <p>The 5th replica shard for the mentioned primary shard will be unassigned, as there is no node available where the primary shard's replicas already reside on.</p> <p>To get the status back to green:</p> <ul> <li>add a data node</li> <li>reduce the replica number</li> </ul>"},{"location":"03-tools/elastic/#lets-see-the-yellow-status-in-action","title":"Let's see the YELLOW Status in action:","text":"<p>Increase the replica shards to <code>5</code>:</p> <pre><code>$ curl -XPUT http://elasticsearch:9200/my2ndindex/_settings -d '{\"settings\": {\"number_of_replicas\": 5}}'\n{\"acknowledged\":true}\n</code></pre> <p>Verify the Indices Overview:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/indices/my*?v'\nhealth status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\nyellow open   my2ndindex V32G9IOoTF6uq0DuNUIAMA   5   5          1            0     22.2kb          4.4kb\n</code></pre> <p>We can see that we have a YELLOW status, for more info let's have a look at the shards overview:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/shards/my2ndindex?v\nindex      shard prirep state      docs store ip        node\nmy2ndindex 3     r      STARTED       1 3.9kb 10.0.2.28 ea1q921\nmy2ndindex 3     p      STARTED       1 3.9kb 10.0.2.22 ifgr6ym\nmy2ndindex 3     r      STARTED       1 3.9kb 10.0.2.18 mstWlao\nmy2ndindex 3     r      STARTED       1 3.9kb 10.0.2.27 bDWFHuw\nmy2ndindex 3     r      STARTED       1 3.9kb 10.0.2.24 rNDYCtL\nmy2ndindex 3     r      UNASSIGNED\nmy2ndindex 2     r      STARTED       0  130b 10.0.2.28 ea1q921\nmy2ndindex 2     r      STARTED       0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 2     r      STARTED       0  130b 10.0.2.18 mstWlao\nmy2ndindex 2     r      STARTED       0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 2     p      STARTED       0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 2     r      UNASSIGNED\nmy2ndindex 4     r      STARTED       0  130b 10.0.2.28 ea1q921\nmy2ndindex 4     r      STARTED       0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 4     r      STARTED       0  130b 10.0.2.18 mstWlao\nmy2ndindex 4     p      STARTED       0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 4     r      STARTED       0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 4     r      UNASSIGNED\nmy2ndindex 1     r      STARTED       0  130b 10.0.2.28 ea1q921\nmy2ndindex 1     r      STARTED       0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 1     p      STARTED       0  130b 10.0.2.18 mstWlao\nmy2ndindex 1     r      STARTED       0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 1     r      STARTED       0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 1     r      UNASSIGNED\nmy2ndindex 0     p      STARTED       0  130b 10.0.2.28 ea1q921\nmy2ndindex 0     r      STARTED       0  130b 10.0.2.22 ifgr6ym\nmy2ndindex 0     r      STARTED       0  130b 10.0.2.18 mstWlao\nmy2ndindex 0     r      STARTED       0  130b 10.0.2.27 bDWFHuw\nmy2ndindex 0     r      STARTED       0  130b 10.0.2.24 rNDYCtL\nmy2ndindex 0     r      UNASSIGNED\n</code></pre> <p>Also, when we look at the allocation api, we can see that we have 5 shards that is unassigned:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/allocation?v\nshards disk.indices disk.used disk.avail disk.total disk.percent host      ip        node\n     9       59.2kb    16.8gb     28.8gb     45.7gb           36 10.0.2.18 10.0.2.18 mstWlao\n    10       61.2mb    16.8gb     28.8gb     45.7gb           36 10.0.2.27 10.0.2.27 bDWFHuw\n     9      275.5kb    15.6gb     30.1gb     45.7gb           34 10.0.2.28 10.0.2.28 ea1q921\n     9       64.2mb    15.7gb     29.9gb     45.7gb           34 10.0.2.24 10.0.2.24 rNDYCtL\n    10        3.4mb    15.6gb     30.1gb     45.7gb           34 10.0.2.22 10.0.2.22 ifgr6ym\n     5                                                                               UNASSIGNED\n</code></pre>"},{"location":"03-tools/elastic/#create-index-with-10-primary-shards","title":"Create Index with 10 Primary Shards:","text":"<p>Let's create an index with 10 primary shards and a replica count of 2:</p> <pre><code>$ curl -XPUT http://elasticsearch:9200/my3rdindex -d '{\"settings\": {\"index\": {\"number_of_shards\": 10, \"number_of_replicas\": 2}}}'\n{\"acknowledged\":true,\"shards_acknowledged\":true}/ #\n</code></pre> <p>Verify:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/indices/my*?v'\nhealth status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   my3rdindex ljovpse0RzCB5INxUBLBYg  10   2          0            0      2.4kb           650b\ngreen  open   my2ndindex V32G9IOoTF6uq0DuNUIAMA   5   2          1            0     13.3kb          4.4kb\n</code></pre> <p>View the shard info on our index:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_cat/shards/my3rdindex?v\nindex      shard prirep state   docs store ip        node\nmy3rdindex 8     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy3rdindex 8     p      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy3rdindex 8     r      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy3rdindex 7     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy3rdindex 7     r      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy3rdindex 7     p      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy3rdindex 4     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy3rdindex 4     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy3rdindex 4     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy3rdindex 2     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy3rdindex 2     r      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy3rdindex 2     p      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy3rdindex 5     p      STARTED    0  130b 10.0.2.28 ea1q921\nmy3rdindex 5     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy3rdindex 5     r      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy3rdindex 6     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy3rdindex 6     p      STARTED    0  130b 10.0.2.18 mstWlao\nmy3rdindex 6     r      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy3rdindex 1     r      STARTED    0  130b 10.0.2.28 ea1q921\nmy3rdindex 1     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy3rdindex 1     p      STARTED    0  130b 10.0.2.18 mstWlao\nmy3rdindex 3     p      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy3rdindex 3     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy3rdindex 3     r      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy3rdindex 9     r      STARTED    0  130b 10.0.2.22 ifgr6ym\nmy3rdindex 9     p      STARTED    0  130b 10.0.2.27 bDWFHuw\nmy3rdindex 9     r      STARTED    0  130b 10.0.2.24 rNDYCtL\nmy3rdindex 0     p      STARTED    0  130b 10.0.2.28 ea1q921\nmy3rdindex 0     r      STARTED    0  130b 10.0.2.18 mstWlao\nmy3rdindex 0     r      STARTED    0  130b 10.0.2.24 rNDYCtL\n</code></pre> <p>Take note, with the configuration as above your index that you created will have 30 shards in your cluster:</p> <pre><code>$ curl -s -XGET 'http://elasticsearch:9200/_cat/shards/my3rdindex?v'  | grep -v 'node' | wc -l\n30\n</code></pre> <p>Number of Primary Shards per Node:</p> <pre><code>$ curl -s -XGET 'http://elasticsearch:9200/_cat/shards/my3rdindex?v' | grep 'p      STARTED' | awk '{print $7}' | sort | uniq -c\n      2 10.0.2.18\n      3 10.0.2.22\n      1 10.0.2.24\n      1 10.0.2.27\n      3 10.0.2.28\n</code></pre>"},{"location":"03-tools/elastic/#ingest-documents-into-elasticsearch","title":"Ingest Documents into Elasticsearch:","text":""},{"location":"03-tools/elastic/#structure","title":"Structure:","text":"<p>In Elasticsearch we have <code>Indices</code>, 'Types<code>, and</code>Documents`. In a Relational Database you can think of it like, Database, Tables, Records:</p> <ul> <li>Indices =&gt; Databases</li> <li>Types =&gt; Tables</li> <li>Documents =&gt; Records</li> </ul>"},{"location":"03-tools/elastic/#ingest-a-document-and-specify-the-id","title":"Ingest a Document and Specify the ID:","text":"<p>When you do a <code>PUT</code> request, you need to specify the <code>id</code> of the document:</p> <ul> <li>\"_id\": 1</li> <li>\"_id\": \"james\"</li> </ul> <p>Let's ingest a simple document with a random string as the document id:</p> <pre><code>$ curl -XPUT http://elasticsearch:9200/people/users/abcd -d '{\"name\", \"james\", \"age\": 28}'\n{\"_index\":\"people\",\"_type\":\"users\",\"_id\":\"abcd\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":2,\"failed\":0},\"created\":true}\n</code></pre> <p>If we have to repeat the same request with the same <code>id</code>, the docuement will be overwritten, ES will create a new document if the <code>id</code> is not present.</p> <pre><code>$ curl -XPUT http://elasticsearch:9200/people/users/abcd -d '{\"name\": \"james\", \"age\": 28}'\n{\"_index\":\"people\",\"_type\":\"users\",\"_id\":\"abcd\",\"_version\":2,\"result\":\"updated\",\"_shards\":{\"total\":2,\"successful\":2,\"failed\":0},\"created\":false}\n</code></pre>"},{"location":"03-tools/elastic/#ingeest-a-document-and-let-es-generate-a-id","title":"Ingeest a Document and Let ES generate a ID:","text":"<p>When you do a <code>POST</code> request, the service will automatically assign a <code>id</code> for your docuemt:</p> <pre><code>$ curl -XPOST http://elasticsearch:9200/people/users/ -d '{\"name\": \"susan\", \"age: 30}'\n{\"_index\":\"people\",\"_type\":\"users\",\"_id\":\"AV3H_9q6AH1phg1wCfDW\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":2,\"failed\":0},\"created\":true}\n</code></pre>"},{"location":"03-tools/elastic/#bulk-ingest","title":"Bulk Ingest","text":"<p>Our Sample Data: <code>info.json</code>:</p> <pre><code>{\"index\":{\"_index\":\"info\",\"_type\":\"feed\",\"_id\":1}}\n{\"user_id\":james4,\"handle_name\":\"james\",\"category\":\"sport\",\"socialmedia_src\":\"twitter\",\"text\":\"manchester united lost\",\"country\":\"south africa\"}\n{\"index\":{\"_index\":\"info\",\"_type\":\"feed\",\"_id\":2}}\n{\"user_id\":pete09,\"handle_name\":\"pete\",\"category\":\"politics\",\"socialmedia_src\":\"facebook\",\"text\":\"new mayor selected\",\"country\":\"new zealand\"}\n</code></pre> <p>Ingest using the Bulk Api:</p> <pre><code>curl -XPOST 'http://elasticsearch:9200/info/_bulk?pretty' --data-binary @info.json\n</code></pre>"},{"location":"03-tools/elastic/#mapping","title":"Mapping","text":""},{"location":"03-tools/elastic/#create-mapping","title":"Create Mapping","text":""},{"location":"03-tools/elastic/#view-mappings","title":"View Mappings","text":""},{"location":"03-tools/elastic/#check-fields-in-mappings","title":"Check Fields in Mappings:","text":"<p>Check if a field exisists in your mapping:</p> <pre><code>$ curl -XGET 'http://127.0.0.1:9200/index-name-2018.03.01/_mapping/docs/field/company?pretty'\n{\n  \"index-name-2018.03.01\" : {\n    \"mappings\" : {\n      \"docs\" : {\n        \"company\" : {\n          \"full_name\" : \"company\",\n          \"mapping\" : {\n            \"company\" : {\n              \"type\" : \"text\",\n              \"fields\" : {\n                \"keyword\" : {\n                  \"type\" : \"keyword\",\n                  \"ignore_above\" : 256\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"03-tools/elastic/#open-close-api","title":"Open / Close API:","text":"<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html</li> </ul>"},{"location":"03-tools/elastic/#close-index","title":"Close Index:","text":"<pre><code>$ curl -XPOST http://elasticsearch:9200/people/_close\n{\"acknowledged\":true}\n</code></pre> <p>Trying to ingest while the index is closed:</p> <pre><code>$ curl -XPOST http://elasticsearch:9200/people/users/ -d '{\"name\": \"susan\", \"age\": 30}'\n{\"error\":{\"root_cause\":[{\"type\":\"index_closed_exception\",\"reason\":\"closed\",\"index_uuid\":\"Yt31-EAwTOa-a6duElYRsQ\",\"index\":\"people\"}],\"type\":\"index_closed_exception\",\"reason\":\"closed\",\"index_uuid\":\"Yt31-EAwTOa-a6duElYRsQ\",\"index\":\"people\"},\"status\":403}\n</code></pre>"},{"location":"03-tools/elastic/#open-index","title":"Open Index:","text":"<pre><code>$ curl -XPOST http://elasticsearch:9200/people/_open\n</code></pre>"},{"location":"03-tools/elastic/#searching","title":"Searching","text":""},{"location":"03-tools/elastic/#get-a-document-by-id","title":"Get a Document by ID:","text":"<p>We can get the document by passing the document <code>id</code>:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/people/users/abcd?pretty\n{\n  \"_index\" : \"people\",\n  \"_type\" : \"users\",\n  \"_id\" : \"abcd\",\n  \"_version\" : 2,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"james\",\n    \"age\" : 28\n  }\n}\n</code></pre>"},{"location":"03-tools/elastic/#determine-which-shard-a-document-reside-on","title":"Determine which Shard a Document Reside on:","text":"<pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/people/users/_search?q=age:28&amp;explain&amp;pretty'\n{\n  \"took\" : 73,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 1,\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_shard\" : \"[people][2]\",\n        \"_node\" : \"ea1q921TQWyNiyiRXzfXZQ\",\n        \"_index\" : \"people\",\n        \"_type\" : \"users\",\n        \"_id\" : \"abcd\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"james\",\n          \"age\" : 28\n        },\n        \"_explanation\" : {\n          \"value\" : 1.0,\n          \"description\" : \"age:[28 TO 28], product of:\",\n          \"details\" : [\n            {\n              \"value\" : 1.0,\n              \"description\" : \"boost\",\n              \"details\" : [ ]\n            },\n            {\n              \"value\" : 1.0,\n              \"description\" : \"\n              Norm\",\n              \"details\" : [ ]\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"03-tools/elastic/#search-api","title":"Search API:","text":"<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html</li> </ul> <p>Lets do a search on our index:</p> <pre><code>$ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/people/_search?pretty\n{\n  \"took\" : 29,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"people\",\n        \"_type\" : \"users\",\n        \"_id\" : \"abcd\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"james\",\n          \"age\" : 28\n        }\n      },\n      {\n        \"_index\" : \"people\",\n        \"_type\" : \"users\",\n        \"_id\" : \"AV3H_9q6AH1phg1wCfDW\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"susan\",\n          \"age\" : 30\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>By default the Search API returns 10 items, which can be changed using <code>size</code></p> <pre><code>curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/shakespeare/_search?size=3&amp;pretty'\n{\n  \"took\" : 25,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 111396,\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"shakespeare\",\n        \"_type\" : \"act\",\n        \"_id\" : \"0\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"line_id\" : 1,\n          \"play_name\" : \"Henry IV\",\n          \"speech_number\" : \"\",\n          \"line_number\" : \"\",\n          \"speaker\" : \"\",\n          \"text_entry\" : \"ACT I\"\n        }\n      },\n      {\n        \"_index\" : \"shakespeare\",\n        \"_type\" : \"line\",\n        \"_id\" : \"14\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"line_id\" : 15,\n          \"play_name\" : \"Henry IV\",\n          \"speech_number\" : 1,\n          \"line_number\" : \"1.1.12\",\n          \"speaker\" : \"KING HENRY IV\",\n          \"text_entry\" : \"Did lately meet in the intestine shock\"\n        }\n      },\n      {\n        \"_index\" : \"shakespeare\",\n        \"_type\" : \"line\",\n        \"_id\" : \"19\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"line_id\" : 20,\n          \"play_name\" : \"Henry IV\",\n          \"speech_number\" : 1,\n          \"line_number\" : \"1.1.17\",\n          \"speaker\" : \"KING HENRY IV\",\n          \"text_entry\" : \"The edge of war, like an ill-sheathed knife,\"\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>View the latest indexed document (this will only work if theres a @timestmap field):</p> <p><pre><code>curl -H 'content-type: application/json' -XPOST http://elasticsearch:9200/&lt;timestamped-index&gt;/_search?pretty -d '{\"size\": 1, \"sort\": { \"@timestamp\": \"desc\"}, \"query\": {\"match_all\": {} }}'\n ```\n\n## Query\n\nQuery our index for people with the age of 28:\n</code></pre> curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/people/_search?q=age:30&amp;pretty' {   \"took\" : 25,   \"timed_out\" : false,   \"_shards\" : {     \"total\" : 5,     \"successful\" : 5,     \"failed\" : 0   },   \"hits\" : {     \"total\" : 1,     \"max_score\" : 1.0,     \"hits\" : [       {         \"_index\" : \"people\",         \"_type\" : \"users\",         \"_id\" : \"AV3H_9q6AH1phg1wCfDW\",         \"_score\" : 1.0,         \"_source\" : {           \"name\" : \"susan\",           \"age\" : 30         }       }     ]   } } <pre><code>#### Query by Term and limit results by 2:\n</code></pre> $ curl -XGET http://127.0.0.1:9200/scrape-sysadmins/_search?pretty -d ' {   \"query\": {     \"term\": {       \"title\": \"traefik\"     }   },   \"size\": 2 } ' <pre><code>#### Query by Match:\n</code></pre> $ curl -XGET http://127.0.0.1:9200/scrape-sysadmins/_search?pretty -d ' {   \"query\": {     \"match\": {       \"title\": \"traefik\"     }   },   \"size\": 10 } ' <pre><code>#### Query with Bool:\n\n- Check if field exists in index:\n</code></pre> $ curl http://127.0.0.1:9200/test4/_search?pretty -d ' {   \"query\": {     \"bool\": {       \"must\": [{         \"exists\": {       \"field\": \"name\"     }       }]     }   } }'</p> <p>{   \"took\" : 7,   \"timed_out\" : false,   \"_shards\" : {     \"total\" : 5,     \"successful\" : 5,     \"failed\" : 0   },   \"hits\" : {     \"total\" : 1,     \"max_score\" : 1.0,     \"hits\" : [       {         \"_index\" : \"test4\",         \"_type\" : \"docs\",         \"_id\" : \"2\",         \"_score\" : 1.0,         \"_source\" : {           \"id\" : \"2\",           \"name\" : \"ruan\"         }       }     ]   } } <pre><code>#### Other Examples of Query:\n\nMatch:\n</code></pre> {   \"query\": {     \"match\": {       \"title\": \"something\"     }   } } <pre><code>Multi match with boost on title:\n</code></pre></p>"},{"location":"03-tools/elastic/#boosts-the-score-4-times-on-title","title":"^ boosts the score 4 times on title","text":"<p>{   \"query\": {     \"multi_match\": {       \"query\": \"something\",       \"fields\": [\"title^4\", \"plot\"]     }   } } <pre><code>Match phrase:\n</code></pre> {   \"query\": {     \"match_phrase\": {       \"title\": \"somethings got to give\"     }   } } <pre><code>Common terms:\n</code></pre> {   \"query\": {     \"common\": {       \"title\": {         \"query\": \"the something word\"       }     }   } } <pre><code>Query string:\n</code></pre> {   \"query\": {     \"query_string\": {       \"query\": \"the something AND (gives OR gave)\"     }   } } <pre><code>Simple query string:\n</code></pre> {   \"query\": {     \"simple_query_string\": {       \"query\": \"\\\"give got to\\\"~4 | *thing~2\",       \"fields\": [\"title\"]     }   } } <pre><code>More info on above:\n</code></pre> +    -&gt; Acts as the AND operator |    -&gt; Acts as the OR operator *    -&gt; Acts as a wildcard. \"\"   -&gt; Wraps several terms into a phrase. ()   -&gt; Wraps a clause for precedence. ~n   -&gt; When used after a term (e.g. thign~3), sets fuzziness. When used after a phrase, sets slop. See Options. -    -&gt; Negates the term. <pre><code>Match all:\n</code></pre> {   \"query\": {     \"match_all\": {}   } } <pre><code>Match none:\n</code></pre> {   \"query\": {     \"match_none\": {}   } } <pre><code>- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html\n\n## Sort\n- https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html\n\nSort Per Field:\n\nIngest a couple of example documents:\n</code></pre> $ curl -XPUT http://elasticsearch:9200/products/items/1 -d '{\"product\": \"chocolate\", \"price\": [20, 4]}' $ curl -XPUT http://elasticsearch:9200/products/items/2 -d '{\"product\": \"apples\", \"price\": [28, 6]}' $ curl -XPUT http://elasticsearch:9200/products/items/3 -d '{\"product\": \"bananas\", \"price\": [28, 22, 23, 20]}' $ curl -XPUT http://elasticsearch:9200/products/items/4 -d '{\"product\": \"chips\", \"price\": [14, 24, 22, 12]}' <pre><code>Run a Sort Query on the term `bananas`, and show the `average` price. We can also use `min, max, avg, sum`:\n</code></pre> $ curl -XPOST http://elasticsearch:9200/products/_search?pretty -d ' {   \"query\" : {     \"term\" : {       \"product\" : \"bananas\"     }   },   \"sort\" : [{     \"price\" : {       \"order\" : \"asc\",       \"mode\" : \"avg\"     }   }] }'</p> <p>{   \"took\" : 9,   \"timed_out\" : false,   \"_shards\" : {     \"total\" : 5,     \"successful\" : 5,     \"failed\" : 0   },   \"hits\" : {     \"total\" : 1,     \"max_score\" : null,     \"hits\" : [       {         \"_index\" : \"products\",         \"_type\" : \"items\",         \"_id\" : \"3\",         \"_score\" : null,         \"_source\" : {           \"product\" : \"bananas\",           \"price\" : [             28,             22,             23,             20           ]         },         \"sort\" : [           23         ]       }     ]   } } <pre><code>Running the same, but wanting to see the sum of all the prices:\n</code></pre> $ curl -XPOST http://elasticsearch:9200/products/_search?pretty -d ' {   \"query\" : {     \"term\" : {       \"product\" : \"bananas\"     }   },   \"sort\" : [{     \"price\" : {       \"order\" : \"asc\",       \"mode\" : \"sum\"     }   }] }'</p> <p>{   \"took\" : 34,   \"timed_out\" : false,   \"_shards\" : {     \"total\" : 5,     \"successful\" : 5,     \"failed\" : 0   },   \"hits\" : {     \"total\" : 1,     \"max_score\" : null,     \"hits\" : [       {         \"_index\" : \"products\",         \"_type\" : \"items\",         \"_id\" : \"3\",         \"_score\" : null,         \"_source\" : {           \"product\" : \"bananas\",           \"price\" : [             28,             22,             23,             20           ]         },         \"sort\" : [           93         ]       }     ]   } } <pre><code># Delete\n\nReferences:\n\n- [Delete API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete.html)\n- [Delete by Query](https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-delete-by-query.html)\n\n## Delete Index:\n</code></pre> $ curl -XDELETE http://elasticsearch:9200/myindex <pre><code>## Delete Documents on Query:\n\nWe would like to delete all documents that has `\"os_name\": \"Windows 10\"`\n</code></pre> curl -XPOST 'http://elasticsearch:9200/weblogs/_delete_by_query?pretty' -d ' {   \"query\": {     \"match\": {       \"os_name\": \"Windows 10\"     }   } }'</p> <p>{   \"took\" : 1217,   \"timed_out\" : false,   \"total\" : 48,   \"deleted\" : 48,   \"batches\" : 1,   \"version_conflicts\" : 0,   \"noops\" : 0,   \"retries\" : {     \"bulk\" : 0,     \"search\" : 0   },   \"throttled_millis\" : 0,   \"requests_per_second\" : -1.0,   \"throttled_until_millis\" : 0,   \"failures\" : [ ] } <pre><code>If routing is provided, then the routing is copied to the scroll query, limiting the process to the shards that match that routing value:\n</code></pre> $ curl -XPOST 'http://elasticsearch:9200/people/_delete_by_query?routing=1 {   \"query\": {     \"range\" : {         \"age\" : {            \"gte\" : 10         }     }   } } <pre><code>By default _delete_by_query uses scroll batches of 1000. You can change the batch size with the scroll_size URL parameter:\n</code></pre> $ curl -XPOST 'http://elasticsearch:9200/weblogs/_delete_by_query?scroll_size=5000 {   \"query\": {     \"term\": {       \"category\": \"docker\"     }   } } <pre><code>## Delete Stats:\n</code></pre> $ curl -XGET 'elasticsearch:9200/_tasks?detailed=true&amp;actions=*/delete/byquery&amp;pretty' {   \"nodes\" : {     \"s5A2CoRWrwKf512z6NEscF\" : {       \"name\" : \"r4A5VoT\",       \"transport_address\" : \"127.0.0.1:9300\",       \"host\" : \"127.0.0.1\",       \"ip\" : \"127.0.0.1:9300\",       \"attributes\" : {         \"testattr\" : \"test\",         \"portsfile\" : \"true\"       },       \"tasks\" : {         \"s5A2CoRWrwKf512z6NEscF\" : {           \"node\" : \"s5A2CoRWrwKf512z6NEscF\",           \"id\" : 36619,           \"type\" : \"transport\",           \"action\" : \"indices:data/write/delete/byquery\",           \"status\" : {             \"total\" : 6154,             \"updated\" : 0,             \"created\" : 0,             \"deleted\" : 3500,             \"batches\" : 36,             \"version_conflicts\" : 0,             \"noops\" : 0,             \"retries\": 0,             \"throttled_millis\": 0           },           \"description\" : \"\"         }       }     }   } } <pre><code># Snapshots\n\n## Elasticsearch S3 Snapshot Repo\n\nSetup the [S3 Snapshot Repository](https://sysadmins.co.za/aws-elasticsearch-register-s3-repository-for-snapshots-using-the-cli/?rbas_source=gist.github.com?rbas_sourcepage=cheatsheet-elasticsearch.md)\n\nList the Snapshot Repositories:\n</code></pre> $ curl -XGET 'http://127.0.0.1:9200/_cat/repositories?v' id            type foo-bacups    s3 bar-backups   s3 <pre><code>View the Snapshot Repository:\n</code></pre> $ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_snapshot/bar-backups?pretty' {   \"bar-backups\" : {     \"type\" : \"s3\",     \"settings\" : {       \"bucket\" : \"my-es-snapshot-bucket\",       \"region\" : \"eu-west-1\",       \"role_arn\" : \"arn:aws:iam::0123456789012:role/elasticsearch-snapshot-role\"     }   } } <pre><code>## Elasticsearch Snapshots\n\nCreate a Snapshot named `mysnapshot_ruan-test-2018-05-24_1` of the index: `ruan-test-2018-05-24` and return the exit when the snapshot is done:\n</code></pre> $ curl -XPUT -H 'Content-Type: application/json' \\   'http://elasticsearch:9200/_snapshot/bar-backups/mysnapshot_ruan-test-2018-05-24_1?wait_for_completion=true&amp;pretty=true' -d ' {     \"indices\": \"ruan-test-2018-05-24\",     \"ignore_unavailable\": true,     \"include_global_state\": false } '</p> <p>{   \"snapshot\" : {     \"snapshot\" : \"mysnapshot_ruan-test-2018-05-24_1\",     \"uuid\" : \"YRTE5922QCeqyEaMxPqb1A\",     \"version_id\" : 6000199,     \"version\" : \"6.0.1\",     \"indices\" : [ \"ruan-test-2018-05-24\" ],     \"state\" : \"SUCCESS\",     \"start_time\" : \"2018-05-25T13:20:11.497Z\",     \"start_time_in_millis\" : 1527254411497,     \"end_time\" : \"2018-05-25T13:20:11.886Z\",     \"end_time_in_millis\" : 1527254411886,     \"duration_in_millis\" : 389,     \"failures\" : [ ],     \"shards\" : {       \"total\" : 5,       \"failed\" : 0,       \"successful\" : 5     }   } } <pre><code>Verify the Snapshot:\n</code></pre> $ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/cat/snapshots/bar-backups?v&amp;s=id' id                                      status start_epoch start_time end_epoch  end_time duration indices successful_shards failed_shards total_shards mysnapshot_ruan-test-2018-05-24_1      SUCCESS 1527254411  06:20:11   1527254411 06:20:11    389ms       1                 5             0            5 <pre><code>## Elasticsearch Restore\n\nGet the Metadata of the Snapshot:\n</code></pre> $ curl -sSL -u \"$ES_USER\":\"$ES_PASS\" \"$ES_URL/_snapshot/bar-backups/mysnapshot_ruan-test-2018-05-24_1?pretty' {   \"snapshots\" : [ {     \"snapshot\" : \"mysnapshot_ruan-test-2018-05-24_1\",     \"uuid\" : \"YRTE5922QCeqyEaMxPqb1A\",     \"version_id\" : 6000199,     \"version\" : \"6.0.1\",     \"indices\" : [ \"ruan-test-2018-05-24\" ],     \"state\" : \"SUCCESS\",     \"start_time\" : \"2018-05-25T13:20:11.497Z\",     \"start_time_in_millis\" : 1527254411497,     \"end_time\" : \"2018-05-25T13:20:11.886Z\",     \"end_time_in_millis\" : 1527254411886,     \"duration_in_millis\" : 389,     \"failures\" : [ ],     \"shards\" : {       \"total\" : 5,       \"failed\" : 0,       \"successful\" : 5     }   } ] } <pre><code>Inspect the Snapshot on S3:\n</code></pre> $ aws s3 --profile es ls s3://my-es-snapshot-bucket/ | grep VRTF2942QCeqyEaMxPgb1B 2018-05-25 15:20:12         90 meta-VRTF2942QCeqyEaMxPgb1B.dat 2018-05-25 15:20:12        258 snap-VRTF2942QCeqyEaMxPgb1B.dat <pre><code>Execute the Restore:\n</code></pre> $ curl -XPOST -H 'Content-Type: application/json' 'http://elasticsearch:9200/_snapshot/bar-backups/mysnapshot_ruan-test-2018-05-24_1/_restore -d ' {   \"indices\": \"ruan-test-2018-05-24\",   \"ignore_unavailable\": true,   \"include_global_state\": false,   \"rename_pattern\": \"index(.+)\",   \"rename_replacement\": \"restored_index_$1\" } ' ```</p> <p>or leave out the body for normal restore</p>"},{"location":"03-tools/elastic/#elasticsearch-snapshot-resources","title":"Elasticsearch Snapshot Resources:","text":"<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html</li> <li>https://www.youtube.com/watch?v=Otl-IcmbiDE</li> <li>https://recology.info/2015/02/elasticsearch-backup-restore/</li> <li>https://medium.com/@rcdexta/periodic-snapshotting-of-elasticsearch-indices-f6b6ca221a0c</li> </ul>"},{"location":"03-tools/flux/99-runbooks/","title":"99 runbooks","text":""},{"location":"03-tools/flux/99-runbooks/#helm-rollback-failed","title":"Helm rollback failed","text":""},{"location":"03-tools/flux/99-runbooks/#meaning","title":"Meaning","text":"<p>There is an existing issue with the Flux helm-controller (https://github.com/fluxcd/helm-controller/issues/149) that can cause HelmReleases to get \"stuck\" with an error message like \"Helm upgrade failed: another operation (install/upgrade/rollback) is in progress\". This can happen anytime the helm-controller is restarted while a HelmRelease is upgrading/installing/etc.</p>"},{"location":"03-tools/flux/99-runbooks/#mitigation","title":"Mitigation","text":"<p>To ensure the HelmRelease error was caused by the helm-controller restarting, first try to suspend/resume the HelmRelease:</p> <pre><code>flux suspend hr &lt;helmrelease&gt; -n &lt;namespace&gt;\nflux resume hr &lt;helmrelease&gt; -n &lt;namespace&gt;\n</code></pre> <p>You can find any flux resource that is not ready by running</p> <pre><code>flux get all -A --status-selector ready=false\n</code></pre>"},{"location":"03-tools/flux/99-runbooks/#scenario-a","title":"Scenario A","text":"<p>After applying suspend/resume the issue is solved and a message with \"Release reconciliation succeeded\" appears.</p> <p>Example</p> <pre><code>\u279c  ~ flux get all -A --status-selector ready=false\nNAMESPACE       NAME    REVISION        SUSPENDED       READY   MESSAGE\n\nNAMESPACE       NAME    REVISION        SUSPENDED       READY   MESSAGE\n\nNAMESPACE       NAME    REVISION        SUSPENDED       READY   MESSAGE\n\nNAMESPACE       NAME    REVISION        SUSPENDED       READY   MESSAGE\n\nNAMESPACE       NAME    REVISION        SUSPENDED       READY   MESSAGE\n\nNAMESPACE       NAME                    REVISION        SUSPENDED       READY   MESSAGE\nmonitoring      helmrelease/gatus       1.5.1           True            False   Helm rollback failed: release gatus failed: context deadline exceeded\n\n                                                                                Last Helm logs:\n\n                                                                                preparing rollback of gatus\n                                                                                rolling back gatus (current: v2, target: v1)\n                                                                                creating rolled back release for gatus\n                                                                                performing rollback of gatus\n</code></pre>"},{"location":"03-tools/flux/99-runbooks/#scenario-b","title":"Scenario B","text":"<p>or it will fail with the same error as before, i.e: \"Helm upgrade failed: another operation (install/upgrade/rollback) is in progress\".</p> <p>If the HelmRelease is still in the failed state, it's likely related to the helm-controller restarting. To resolve the issue, do the following.</p> <ol> <li>List secrets containing the affected HelmRelease name.</li> </ol> <p>Example</p> <pre><code>sh.helm.release.v1.blackbox-exporter.v1         helm.sh/release.v1                    1      13d\nsh.helm.release.v1.gatus.v1                     helm.sh/release.v1                    1      13d\nsh.helm.release.v1.gatus.v2                     helm.sh/release.v1                    1      6d13h\nsh.helm.release.v1.gatus.v3                     helm.sh/release.v1                    1      6d13h\nsh.helm.release.v1.gatus.v4                     helm.sh/release.v1                    1      6m46s\nsh.helm.release.v1.gatus.v5                     helm.sh/release.v1                    1      5m30s\n</code></pre> <ol> <li>Find and delete the most recent revision secret (i.e. <code>sh.helm.release.v1.*.&lt;revision&gt;</code>)</li> </ol> <p>Example</p> <pre><code>kubectl get secrets -n monitoring | grep gatus | awk '{ print $1 }' | xargs -I{} sh -c \"kubectl delete secret -n monitoring {}\n</code></pre> <ol> <li>suspend/resume the HelmRelease to trigger a reconciliation</li> </ol> <p>Example</p> <pre><code>flux resume hr gatus -n monitoring\n</code></pre> <ol> <li>You should see the HelmRelease get reconciled then eventually the upgrade/install will succeed.</li> </ol>"},{"location":"03-tools/flux/99-runbooks/#additional-information","title":"Additional information","text":"<ul> <li>https://support.d2iq.com/hc/en-us/articles/8295311458964-Resolving-issues-with-HelmReleases-that-are-failed-</li> <li>https://github.com/fluxcd/helm-controller/issues/149</li> </ul>"},{"location":"03-tools/flux/99-runbooks/#manually-rollback-to-a-previous-revision","title":"Manually rollback to a previous revision","text":"<ol> <li>List all the releases</li> <li>Check the history for that release</li> <li>Rollback the release to a previous revision</li> </ol> <pre><code>helm list -Aa\nhelm history &lt;release&gt; -n &lt;name-space&gt;\nhelm rollback &lt;release&gt; &lt;revision&gt; -n &lt;name-space&gt;\n</code></pre>"},{"location":"03-tools/kubernetes/delete_namespace_stucked/","title":"How do I troubleshoot namespaces in a terminated state?","text":""},{"location":"03-tools/kubernetes/delete_namespace_stucked/#short-description","title":"Short description","text":"<p>To delete a namespace, Kubernetes must first delete all the resources in the namespace. Then, it must check registered API services for the status. A namespace gets stuck in Terminating stats for the following reasons:</p> <ul> <li>The namespace contains resources that Kubernetes can't delete</li> <li>An API service has a False status</li> </ul>"},{"location":"03-tools/kubernetes/delete_namespace_stucked/#resolution","title":"Resolution","text":"<ol> <li>Save JSON file like in the following example:</li> </ol> <pre><code>kubectl get namespace TERMINATING_NAMESPACE -o json &gt; tempfile.json\n</code></pre> <p>Note: Replace TERMINATING_NAMESPACE with the name of your stuck namespace.</p> <ol> <li>Remove the finalizers array block from the spec section of the JSON file:</li> </ol> <pre><code>\"spec\": {\n        \"finalizers\": [\n            \"kubernetes\"\n        ]\n    }\n</code></pre> <p>After you remove the finalizers array block, the spec section of the JSON file looks like this:</p> <pre><code>\"spec\" : {\n    }\n</code></pre> <ol> <li>To apply the changes, run the following command:</li> </ol> <pre><code>kubectl replace --raw \"/api/v1/namespaces/TERMINATING_NAMESPACE/finalize\" -f ./tempfile.json\n</code></pre> <p>Note: Replace TERMINATING_NAMESPACE with the name of your stuck namespace.</p> <ol> <li>Verify that the terminating namespace is removed:</li> </ol> <pre><code>kubectl get namespaces\n</code></pre>"},{"location":"03-tools/kubernetes/delete_namespace_stucked/#related-information","title":"Related information","text":"<ul> <li>Knowledge Center - How do I troubleshoot namespaces in a termianted state in my Amazon EKS Cluster?</li> </ul>"},{"location":"03-tools/kubernetes/storage/","title":"Storage","text":""},{"location":"03-tools/kubernetes/storage/#set-a-storage-class-as-default","title":"Set a Storage Class as Default","text":"<pre><code>kubectl patch storageclass &lt;storage-class-name&gt; -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre>"},{"location":"archive/03-prometheus/","title":"03 prometheus","text":"<pre><code>prometheus:\n  prometheusSpec:\n    podMonitorSelectorNilUsesHelmValues: false\n    ruleSelectorNilUsesHelmValues: false\n    serviceMonitorSelectorNilUsesHelmValues: false\n    probeSelectorNilUsesHelmValues: false\n</code></pre>"},{"location":"archive/03-prometheus/#servicemonitor-not-showing-up-in-targets","title":"ServiceMonitor not showing up in targets","text":"<p>Info</p> <p>Check the oficial troubleshooting guide</p> <p>Info</p> <p>Prometheus operator helm values</p> <p>By default, Prometheus instances created with the Helm Kube Prometheus stack have a label <code>release: &lt;prometheus-installed-namespace&gt;</code> in their <code>spec.serviceMonitorSelector.matchLabels</code> field. This means that Prometheus will only monitor services that have this label. To verify this, you can run the command <code>kubectl get prom -Ao yaml</code> and check the <code>spec.serviceMonitorSelector</code> field in the output. This means that if a servicemonitor does not have this label, Prometheus created by operator will not monitor it. Check the values.yaml of the kube-prometheus helm chart project.</p> <p>You have two options to get it work without adding <code>release</code> label:</p> <ul> <li> <p>Set <code>serviceMonitorSelectorNilUsesHelmValues</code> to <code>false</code>, the Prometheus will select all the serviceMonitors.</p> <p>before</p> <pre><code>$ kubectl get prom -Ao yaml | grep -A2 -B2 serviceMonitorSelector\nserviceAccountName: kube-prometheus-stack-prometheus\nserviceMonitorNamespaceSelector: {}\nserviceMonitorSelector:\n  matchLabels:\n    release: kube-prometheus-stack\n</code></pre> <p>after</p> <pre><code>$ kubectl get prom -Ao yaml | grep -A2 -B2 serviceMonitorSelector\nserviceAccountName: kube-prometheus-stack-prometheus\nserviceMonitorNamespaceSelector: {}\nserviceMonitorSelector: {}\nshards: 1\nversion: v2.41.0\n</code></pre> <p>moreover</p> <pre><code>$ kubectl -n monitoring get secret prometheus-kube-prometheus-stack-prometheus -ojson | jq -r '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | grep serviceMoni\n- job_name: serviceMonitor/kube-system/descheduler-servicemonitor/0\n- job_name: serviceMonitor/logging/prometheus-elasticsearch-exporter-monitor/0\n- job_name: serviceMonitor/logging/redis/0\n- job_name: serviceMonitor/monitoring/grafana/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-alertmanager/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-apiserver/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-coredns/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-kube-state-metrics/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-kubelet/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-kubelet/1\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-kubelet/2\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-operator/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-prometheus/0\n- job_name: serviceMonitor/monitoring/kube-prometheus-stack-prometheus-node-exporter/0\n</code></pre> </li> <li> <p>Set <code>serviceMonitorSelector</code>to any label you like. Like this</p> </li> </ul> <pre><code>commonLabels:\nprometheus: myLabe\nprometheus:\n  prometheusSpec:\n    serviceMonitorSelector:\n      matchLabels:\n        prometheus: myLabel\n</code></pre>"},{"location":"archive/03-prometheus/#podmonitor-not-showing-up-in-targets","title":"podMonitor not showing up in targets","text":"<p>Info</p> <p>Read the \"serviceMonitor not showing up in targets\" to get a detailed explanation of what happens.</p> <p>Check the output of the following command:</p> <p>before the fix</p> <pre><code>$ kubectl get prom -Ao yaml | grep -A3 -B3 podMonitor\n    logFormat: logfmt\n    logLevel: info\n    paused: false\n    podMonitorNamespaceSelector: {}\n    podMonitorSelector:\n      matchLabels:\n        release: kube-prometheus-stack\n    portName: http-web\n</code></pre> <p>after the fix</p> <pre><code>$ kubectl get prom -Ao yaml | grep -A3 -B3 podMonitor\n    logFormat: logfmt\n    logLevel: info\n    paused: false\n    podMonitorNamespaceSelector: {}\n    podMonitorSelector: {}\n    portName: http-web\n    probeNamespaceSelector: {}\n    probeSelector:\n</code></pre> <pre><code>$ kubectl get prom -Ao yaml | grep -A3 -B3 ruleSelector\n    retentionSize: 15GB\n    routePrefix: /\n    ruleNamespaceSelector: {}\n    ruleSelector:\n      matchLabels:\n        release: kube-prometheus-stack\n    scrapeInterval: 60s\n</code></pre>"},{"location":"archive/03-vector/","title":"03 vector","text":""},{"location":"archive/03-vector/#pipeline-debugging","title":"Pipeline debugging","text":"<ul> <li>Pipeline debugging using vector tap</li> </ul>"},{"location":"archive/aws/","title":"Aws","text":""},{"location":"archive/aws/#create-s3-bucket","title":"Create S3 Bucket","text":"<p>Create an S3 bucket, replacing placeholders appropiately</p> <p>Tip</p> <p>Generate a random string with <code>openssl rand -hex 18</code></p> <pre><code>BUCKET=&lt;YOUR_BUCKET&gt;\nREGION=&lt;YOUR_REGION&gt;\nAPP=&lt;YOUR_APP&gt;\naws s3api create-bucket \\\n    --bucket $BUCKET \\\n    --region $REGION \\\n    --create-bucket-configuration LocationConstraint=$REGION\n</code></pre> <p>NOTE:  us-east-1 does not support a <code>LocationConstraint</code>. If your region is <code>us-east-1</code>, omit the bucket configuration:</p> <pre><code>BUCKET=&lt;YOUR_BUCKET&gt;\nREGION=&lt;YOUR_REGION&gt;\naws s3api create-bucket \\\n    --bucket $BUCKET \\\n    --region us-east-1\n</code></pre>"},{"location":"archive/aws/#set-permissions-for-the-app-with-an-iam-user","title":"Set permissions for the App with an IAM user","text":"<p>Info</p> <p>The permissions for the app can be configured also using kube2iam method.</p> <p>For more information, see the AWS documentation on IAM users</p> <ol> <li>Create the IAM user:</li> </ol> <pre><code>aws iam create-user --user-name $APP\n</code></pre> <ol> <li>Attach policies to give <code>$APP</code> the necessary permissions</li> </ol> <pre><code>cat &gt; $APP-policy.json &lt;&lt;EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeSnapshots\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:CreateSnapshot\",\n                \"ec2:DeleteSnapshot\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::${BUCKET}/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::${BUCKET}\"\n            ]\n        }\n    ]\n}\nEOF\n</code></pre> <pre><code>aws iam put-user-policy \\\n  --user-name $APP \\\n  --policy-name $APP \\\n  --policy-document file://$APP-policy.json\n</code></pre> <ol> <li>Create an access key for the user:</li> </ol> <pre><code>aws iam create-access-key --user-name $APP\n</code></pre> <p>The result should look like</p> <pre><code>{\n  \"AccessKey\": {\n        \"UserName\": \"velero\",\n        \"Status\": \"Active\",\n        \"CreateDate\": \"2017-07-31T22:24:41.576Z\",\n        \"SecretAccessKey\": &lt;AWS_SECRET_ACCESS_KEY&gt;,\n        \"AccessKeyId\": &lt;AWS_ACCESS_KEY_ID&gt;\n  }\n}\n</code></pre>"},{"location":"archive/cloudflare/","title":"Cloudflare","text":""},{"location":"archive/cloudflare/#list-zones","title":"List Zones","text":"<pre><code>curl -s --request GET --url https://api.cloudflare.com/client/v4/zones --header 'Content-Type: application/json' --header 'X-Auth-Email: ' --header 'X-Auth-Key: ' | jq '.result[].id'\n</code></pre>"},{"location":"archive/cloudflare/#list-dns-records","title":"List DNS Records","text":"<pre><code>curl --request GET \\\n  --url https://api.cloudflare.com/client/v4/zones/zone_identifier/dns_records \\\n  --header 'Content-Type: application/json' \\\n  --header 'X-Auth-Email: ' \\\n  --header 'X-Auth-Key:\n</code></pre>"},{"location":"archive/cloudflare/#delete-dns-records","title":"Delete DNS Records","text":"<pre><code>#!/bin/bash\n\nZONE_ID=\"...\"\nAUTH_EMAIL=\"$(gopass show personal/homeops/cloudflare-x-auth-email)\"\nAUTH_KEY=\"$(gopass show personal/homeops/CloudflareGlobalAPIKey)\"\n\n# Get the DNS record IDs\nrecord_ids=$(curl -s --request GET \\\n  --url \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"X-Auth-Email: $AUTH_EMAIL\" \\\n  --header \"X-Auth-Key: $AUTH_KEY\" | jq -r '.result[].id')\n\n# Loop through the IDs and fetch details for each DNS record\nfor id in $record_ids; do\n  curl -s --request DELETE \\\n    --url \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$id\" \\\n    --header \"Content-Type: application/json\" \\\n    --header \"X-Auth-Email: $AUTH_EMAIL\" \\\n    --header \"X-Auth-Key: $AUTH_KEY\"\n  echo \"\"  # Add an empty line for separation between records\ndone\n</code></pre>"},{"location":"archive/fluxcd/","title":"Flux CD","text":"<p>Flux CD is a GitOps continuous delivery tool that automatically synchronizes Kubernetes manifests with the desired state in Git repositories. This guide covers the most important Flux CD commands and includes a troubleshooting section.</p>"},{"location":"archive/fluxcd/#checking-flux-components","title":"Checking Flux Components","text":"<pre><code>flux check\n</code></pre>"},{"location":"archive/fluxcd/#adding-flux-components","title":"Adding Flux Components","text":"<pre><code>flux create source git &lt;source-name&gt; --url=&lt;git-url&gt; --branch=&lt;branch&gt; [--interval=&lt;interval&gt;]\nflux create kustomization &lt;kustomization-name&gt; --source=&lt;source-name&gt; --path=&lt;path&gt; [--interval=&lt;interval&gt;]\n</code></pre>"},{"location":"archive/fluxcd/#synchronizing-flux-components","title":"Synchronizing Flux Components","text":"<pre><code>flux reconcile source git &lt;source-name&gt;\nflux reconcile kustomization &lt;kustomization-name&gt;\n</code></pre>"},{"location":"archive/fluxcd/#suspendingresuming-flux-components","title":"Suspending/Resuming Flux Components","text":"<pre><code>flux suspend kustomization &lt;kustomization-name&gt;\nflux resume kustomization &lt;kustomization-name&gt;\n</code></pre>"},{"location":"archive/fluxcd/#troubleshooting-flux-cd","title":"Troubleshooting Flux CD","text":""},{"location":"archive/fluxcd/#check-the-flux-cd-components-status","title":"Check the Flux CD components' status:","text":"<pre><code>flux get sources git\nflux get kustomizations\n</code></pre>"},{"location":"archive/fluxcd/#inspect-flux-cd-logs","title":"Inspect Flux CD logs:","text":"<pre><code>flux logs --level=&lt;error|info|debug&gt;\n</code></pre>"},{"location":"archive/fluxcd/#troubleshoot-individual-resources","title":"Troubleshoot individual resources:","text":"<pre><code>kubectl describe &lt;resource-type&gt; &lt;resource-name&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Example</p> <pre><code>kubectl describe -n flux-system kustomization cluster-apps\n...\nEvents:\n  Type    Reason                   Age                From                  Message\n  ----    ------                   ----               ----                  -------\n  Normal  ReconciliationSucceeded  46m                kustomize-controller  Reconciliation finished in 2.171262291s, next run in 30m0s\n  Normal  Progressing              15m (x2 over 46m)  kustomize-controller  Namespace/flux-system configured\nNamespace/monitoring configured\n  Normal  ReconciliationSucceeded  15m  kustomize-controller  Reconciliation finished in 2.449433766s, next run in 30m0s\n\u279c  home-ops git:(main) \u2717 kubectl describe -n flux-system kustomization cluster-apps\n</code></pre> <p>FluxCD Multi Tenancy on AKS with GitOps</p>"},{"location":"archive/grafana/","title":"Grafana","text":"<pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: default\nresources:\n  #- ./externalsecret.yaml\n  - ./helmrelease.yaml\nconfigMapGenerator:\n  - name: cloudnative-pg-dashboard\n    files:\n      - cloudnative-pg-dashboard.json=https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/grafana-dashboard.json\ngeneratorOptions:\n  disableNameSuffixHash: true\n  annotations:\n    kustomize.toolkit.fluxcd.io/substitute: disabled\n  labels:\n    grafana_dashboard: \"true\"\n</code></pre>"},{"location":"archive/helpful_commands/","title":"Helpful Commands","text":""},{"location":"archive/helpful_commands/#get-kubeconfig-cluster-configuration","title":"Get kubeconfig cluster configuration","text":"<pre><code>kubectl config view --minify --raw\n</code></pre>"},{"location":"archive/helpful_commands/#delete-all-evicted-pods","title":"Delete all evicted pods","text":"<pre><code>kubectl get pods --all-namespaces --field-selector 'status.phase==Failed' -o json | kubectl delete -f -\n</code></pre> <ul> <li>Source</li> </ul>"},{"location":"archive/helpful_commands/#scale-all-deployments-in-a-namespace","title":"Scale all deployments in a Namespace","text":"<pre><code>kubectl scale deployment -n &lt;namespace&gt; --replicas 0 --all\n</code></pre>"},{"location":"archive/helpful_commands/#sops","title":"SOPS","text":""},{"location":"archive/helpful_commands/#to-encrypt-the-file","title":"To encrypt the file","text":"<pre><code>sops --encrypt --age $(cat $SOPS_AGE_KEY_FILE |grep -oP \"public key: \\K(.*)\") --encrypted-regex '^(data|stringData)$' --in-place ./secret.sops.yaml\n</code></pre>"},{"location":"archive/helpful_commands/#to-decrypt-the-file","title":"To decrypt the file","text":"<pre><code>sops --decrypt --age $(cat $SOPS_AGE_KEY_FILE |grep -oP \"public key: \\K(.*)\") --encrypted-regex '^(data|stringData)$' --in-place ./secret.sops.yaml\n</code></pre>"},{"location":"archive/helpful_commands/#hardware","title":"Hardware","text":""},{"location":"archive/helpful_commands/#how-to-get-device-uuid-path","title":"How to get device UUID path","text":"<p>Example for getting the UUID Path for a device with nvme in the name:</p> <pre><code>ls -la /dev/disk/by-id/* | grep nvme\n</code></pre>"},{"location":"archive/install/","title":"Installation","text":""},{"location":"archive/install/#pre-commit","title":"pre-commit","text":"<p>It is advisable to install pre-commit and the pre-commit hooks that come with this repository.</p> <ol> <li> <p>Enable Pre-Commit</p> <pre><code>task precommit:init\n</code></pre> </li> <li> <p>Update Pre-Commit, though it will occasionally make mistakes, so verify its results.</p> <pre><code>task precommit:update\n</code></pre> </li> </ol>"},{"location":"archive/install/#repository-structure","title":"Repository structure","text":"<p>The Git repository contains the following directories under <code>kubernetes</code> and are ordered below by how Flux will apply them.</p> <pre><code>\ud83d\udcc1 kubernetes      # Kubernetes cluster defined as code\n\u251c\u2500\ud83d\udcc1 bootstrap     # Flux installation\n\u251c\u2500\ud83d\udcc1 flux          # Main Flux configuration of repository\n\u2514\u2500\ud83d\udcc1 apps          # Apps deployed into the cluster grouped by namespace\n</code></pre>"},{"location":"archive/install/#setting-up-age","title":"Setting up Age","text":"<p>Here we will create a Age Private and Public key. Using SOPS with Age allows us to encrypt secrets and use them in Ansible and Flux.</p> <ol> <li>Create a Age Private / Public Key</li> </ol> <pre><code>age-keygen -o age.agekey\n</code></pre> <ol> <li>Set up the directory for the Age key and move the Age file to it</li> </ol> <pre><code>mkdir -p ~/.config/sops/age\nmv age.agekey ~/.config/sops/age/keys.txt\n</code></pre> <ol> <li>Export the <code>SOPS_AGE_KEY_FILE</code> variable in your <code>bashrc</code>, <code>zshrc</code> or <code>config.fish</code> and source it, e.g.</li> </ol> <pre><code>export SOPS_AGE_KEY_FILE=~/.config/sops/age/keys.txt\nsource ~/.bashrc\n</code></pre> <ol> <li>Fill out the Age public key in the <code>.config.env</code> under <code>BOOTSTRAP_AGE_PUBLIC_KEY</code>, note the public key should start with <code>age</code> ...</li> </ol>"},{"location":"archive/install/#configuration","title":"Configuration","text":"<p>The <code>.config.env</code> file contains necessary configuration that is needed by Ansible, Terraform and Flux.</p> <ol> <li> <p>Copy the <code>.config.sample.env</code> to <code>.config.env</code> and start filling out all the environment variables.</p> <p>All are required unless otherwise noted in the comments.</p> <pre><code>cp .config.sample.env .config.env\n</code></pre> </li> <li> <p>Once that is done, verify the configuration is correct by running:</p> <pre><code>task verify\n</code></pre> </li> <li> <p>If you do not encounter any errors run start having the script wire up the templated files and place them where they need to be.</p> <pre><code>task configure\n</code></pre> </li> </ol>"},{"location":"archive/install/#preparing-ubuntu-server-with-ansible","title":"Preparing Ubuntu Server with Ansible","text":"<p>Here we will be running a Ansible Playbook to prepare Ubuntu Server for running a Kubernetes cluster.</p> <ol> <li>Ensure you are able to SSH into your nodes from your workstation using a private SSH key without a passphrase. This is how Ansible is able to connect to your remote nodes.</li> </ol> <p>How to configure SSH key-based authentication</p> <ol> <li> <p>Install the Ansible deps</p> <pre><code>task ansible:init\n</code></pre> </li> <li> <p>Verify Ansible can view your config</p> <pre><code>task ansible:list\n</code></pre> </li> <li> <p>Verify Ansible can ping your nodes</p> <pre><code>task ansible:ping\n</code></pre> </li> <li> <p>Run the Fedora Server Ansible prepare playbook</p> <pre><code>task ansible:prepare\n</code></pre> </li> <li> <p>Reboot the nodes</p> <pre><code>task ansible:reboot\n</code></pre> </li> </ol>"},{"location":"archive/install/#installing-k3s-with-ansible","title":"Installing k3s with Ansible","text":"<p>Here we will be running a Ansible Playbook to install k3s with this wonderful k3s Ansible galaxy role. After completion, Ansible will drop a <code>kubeconfig</code> in <code>./kubeconfig</code> for use with interacting with your cluster with <code>kubectl</code>.</p> <p>If you run into problems, you can run <code>task ansible:nuke</code> to destroy the k3s cluster and start over.</p> <ol> <li> <p>Verify Ansible can view your config</p> <pre><code>task ansible:list\n</code></pre> </li> <li> <p>Verify Ansible can ping your nodes</p> <pre><code>task ansible:ping\n</code></pre> </li> <li> <p>Install k3s with Ansible</p> <pre><code>task ansible:install\n</code></pre> </li> <li> <p>Verify the nodes are online</p> <pre><code>task cluster:nodes\n# NAME           STATUS   ROLES                       AGE     VERSION\n# k8s-0          Ready    control-plane,master      4d20h   v1.21.5+k3s1\n# k8s-1          Ready    worker                    4d20h   v1.21.5+k3s1\n</code></pre> </li> </ol>"},{"location":"archive/k3s/","title":"K3s","text":""},{"location":"archive/k3s/#k3s-documentation-references","title":"k3s documentation references","text":"<pre><code>extra_server_args: \"--disable servicelb --disable traefik --etcd-expose-metrics=true --kube-controller-manager-arg=bind-address=0.0.0.0 --kube-proxy-arg=metrics-bind-address=0.0.0.0 --kube-scheduler-arg=bind-address=0.0.0.0\"\nextra_agent_args: \"\"\n</code></pre> <p>https://docs.k3s.io/cli/server</p>"},{"location":"archive/k3s/#k3s-components-monitoring","title":"k3s components monitoring","text":"<p>By default, K3S components (Scheduler, Controller Manager, and Proxy) do not expose their endpoints for metric collection. Their /metrics endpoints are bound to 127.0.0.1, making them accessible only to localhost and preventing remote queries.</p> <p>To modify this behavior, you must provide the following installation arguments for K3S:</p> <pre><code>--kube-controller-manager-arg 'bind-address=0.0.0.0'\n--kube-proxy-arg 'metrics-bind-address=0.0.0.0'\n--kube-scheduler-arg 'bind-address=0.0.0.0\n</code></pre>"},{"location":"archive/loki/","title":"Loki","text":"<p>Assume you have the following JSON log data:</p> <pre><code>{\n  \"timestamp\": \"2023-03-23T10:00:00Z\",\n  \"level\": \"ERROR\",\n  \"message\": \"An error occurred while processing the request\",\n  \"app\": \"my-app\",\n  \"environment\": \"production\",\n  \"status_code\": 500,\n  \"latency\": 2.5\n}\n</code></pre> <p>Now, let's explore some LogQL queries using this JSON log example:</p>"},{"location":"archive/loki/#log-filtering","title":"Log Filtering","text":"<p>Filter logs based on labels or search terms.</p> <pre><code>// Show logs with the label app=\"my-app\"\n{app=\"my-app\"}\n\n// Show logs containing the string \"error\" from the \"my-app\" app\n{app=\"my-app\"} |= \"error\"\n\n// Show logs with status_code 500 from the \"my-app\" app in the \"production\" environment\n{app=\"my-app\", environment=\"production\"} |= \"500\"\n</code></pre>"},{"location":"archive/loki/#log-parsing","title":"Log Parsing","text":"<p>Loki supports parsing logs with various parsers. For JSON logs, use the json parser.</p> <pre><code>// Parse JSON log fields\n{app=\"my-app\"} | json\n</code></pre>"},{"location":"archive/loki/#log-filtering-with-parsed-fields","title":"Log Filtering with Parsed Fields","text":"<p>After parsing log lines, you can filter based on the parsed fields.</p> <pre><code>// Filter logs with a specific status code (e.g., 500)\n{app=\"my-app\"} | json | status_code == 500\n\n// Filter logs with latency greater than 1 second\n{app=\"my-app\"} | json | latency &gt; 1s\n</code></pre>"},{"location":"archive/loki/#aggregations-and-metrics","title":"Aggregations and Metrics","text":"<p>Generate metrics from logs using various aggregation functions.</p> <pre><code>// Calculate the 99th percentile latency for each unique label set in the last 10 minutes\nquantile_over_time(0.99, {app=\"my-app\"} | json | unwrap latency [10m])\n\n// Count log lines with errors per stream over the last 5 minutes\nsum by (app) (count_over_time({app=\"my-app\"} | json | level == \"ERROR\" [5m]))\n\n// Calculate the average latency for each unique label set in the last 10 minutes\navg_over_time({app=\"my-app\"} | json | unwrap latency [10m])\n</code></pre>"},{"location":"archive/networking/","title":"Networking","text":"<p>Tip</p> <p>If you have any doubts, please refer to the template's documentation. Some sections may have been replicated</p>"},{"location":"archive/networking/#global-cloudflare-api-key","title":"Global Cloudflare API Key","text":"<p>In order to use Terraform and <code>cert-manager</code> with the Cloudflare DNS challenge you will need to create a API key.</p> <ol> <li> <p>Head over to Cloudflare and create a API key by going here.</p> </li> <li> <p>Under the <code>API Keys</code> section, create a global API Key.</p> </li> <li> <p>Use the API Key in the appropriate variable in configuration section below.</p> </li> </ol> <p>You may wish to update this later on to a Cloudflare API Token which can be scoped to certain resources. I do not recommend using a Cloudflare API Key, however for the purposes of this template it is easier getting started without having to define which scopes and resources are needed. For more information see the Cloudflare docs on API Keys and Tokens.</p>"},{"location":"archive/networking/#configuring-cloudflare-dns-with-terraform","title":"Configuring Cloudflare DNS with Terraform","text":"<p>Review the Terraform scripts under <code>./terraform/cloudflare/</code> and make sure you understand what it's doing (no really review it).</p> <p>If your domain already has existing DNS records be sure to export those DNS settings before you continue.</p> <ol> <li> <p>Pull in the Terraform deps</p> <pre><code>task terraform:init\n</code></pre> </li> <li> <p>Review the changes Terraform will make to your Cloudflare domain</p> <pre><code>task terraform:plan\n</code></pre> </li> <li> <p>Have Terraform apply your Cloudflare settings</p> <pre><code>task terraform:apply\n</code></pre> </li> </ol> <p>If Terraform was ran successfully you can log into Cloudflare and validate the DNS records are present.</p> <p>The cluster application external-dns will be managing the rest of the DNS records you will need.</p>"},{"location":"archive/networking/#dns","title":"DNS","text":"<p>The external-dns application created in the <code>networking</code> namespace will handle creating public DNS records. By default, <code>echo-server</code> and the <code>flux-webhook</code> are the only public domain exposed on your Cloudflare domain. In order to make additional applications public you must set an ingress annotation (<code>external-dns.alpha.kubernetes.io/target</code>) like done in the <code>HelmRelease</code> for <code>echo-server</code>. You do not need to use Terraform to create additional DNS records unless you need a record outside the purposes of your Kubernetes cluster (e.g. setting up MX records).</p> <p>k8s_gateway is deployed on the IP choosen for <code>${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR}</code>. Inorder to test DNS you can point your clients DNS to the <code>${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR}</code> IP address and load <code>https://hajimari.${BOOTSTRAP_CLOUDFLARE_DOMAIN}</code> in your browser.</p> <p>You can also try debugging with the command <code>dig</code>, e.g. <code>dig @${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR} hajimari.${BOOTSTRAP_CLOUDFLARE_DOMAIN}</code> and you should get a valid answer containing your <code>${BOOTSTRAP_METALLB_INGRESS_ADDR}</code> IP address.</p> <p>If your router (or Pi-Hole, Adguard Home or whatever) supports conditional DNS forwarding (also know as split-horizon DNS) you may have DNS requests for <code>${SECRET_DOMAIN}</code> only point to the  <code>${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR}</code> IP address. This will ensure only DNS requests for <code>${SECRET_DOMAIN}</code> will only get routed to your k8s_gateway service thus providing DNS resolution to your cluster applications/ingresses.</p> <p>To access services from the outside world port forwarded <code>80</code> and <code>443</code> in your router to the <code>${BOOTSTRAP_METALLB_INGRESS_ADDR}</code> IP, in a few moments head over to your browser and you should be able to access <code>https://echo-server.${BOOTSTRAP_CLOUDFLARE_DOMAIN}</code> from a device outside your LAN.</p> <p>To avoid directly exposing the local cluster, Cloudflare tunnels can be used across the wide area network (WAN). The Cloudflare Operator is employed to establish these tunnels and create Cloudflare DNS entries for services you want to make publicly accessible. Cloudflare serves as a proxy to conceal your home's WAN IP address and functions as a firewall. When you are not connected to your home network, all incoming traffic to your cluster is routed through a Cloudflare tunnel.</p> <p>Warning</p> <p>If you want to follow the cloudflare tunnel approach disable <code>external-dns</code> and <code>cloudflare-ddns</code> components in the kustomization file in the networking namespace.</p>"},{"location":"archive/networking/#cloudflare-operator","title":"Cloudflare Operator","text":"<p>Follow the getting started guide to use cloudflare operator.</p> <p>Warning</p> <p>Notice that ExternalSecrets will try to pull the following secrets from Doppler.</p> <pre><code># kubernetes/apps/networking/cloudflare-operator/app/externalsecrets.yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: cloudflare-operator\n  namespace: cloudflare-operator-system\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: cloudflare-operator-cluster-secret-store\n  target:\n    name: cloudflare-operator-secret\n    creationPolicy: Owner\n    template:\n      engineVersion: v2\n      data:\n        CLOUDFLARE_API_TOKEN: \"{{ .CLOUDFLARE_TOKEN }}\"\n        CLOUDFLARE_API_KEY: \"{{ .CLOUDFLARE_APIKEY }}\"\n\n  data:\n    - secretKey: CLOUDFLARE_TOKEN\n      remoteRef:\n        key: CLOUDFLARE_TOKEN\n\n  - secretKey: CLOUDFLARE_APIKEY\n    remoteRef:\n      key: CLOUDFLARE_APIKEY\n</code></pre>"},{"location":"archive/prometheus/","title":"Prometheus","text":""},{"location":"archive/prometheus/#service-monitoring","title":"Service Monitoring","text":""},{"location":"archive/prometheus/#service-monitor","title":"Service Monitor","text":"<ul> <li> <p>Service Monitor API Docs</p> </li> <li> <p>Retrieve all service monitor available</p> </li> </ul> <pre><code>group by (scrape_job) ({scrape_job!=\"\"})\n</code></pre>"},{"location":"archive/prometheus/#examples","title":"Examples","text":"The service and service monitor are both in the <code>default</code> namespace (click to expand) <pre><code>  ---\n  apiVersion: v1\n  kind: Service\n  metadata:\n    name: my-service\n    namespace: default\n    labels:\n      app: my-app\n\n  ---\n  apiVersion: monitoring.coreos.com/v1\n  kind: ServiceMonitor\n  metadata:\n    name: my-service-monitor\n    namespace: default\n    labels:\n      app: my-app\n  spec:\n    selector:\n      matchLabels:\n        app: my-app\n    endpoints:\n    - path: /metrics\n</code></pre> The service is in the <code>production</code> namespace and service monitor is in <code>monitoring</code> namespace (click to expand) <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: production\n  labels:\n    app: my-app\n\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: my-service-monitor\n  namespace: monitoring\n  labels:\n    app: my-app\nspec:\n  namespaceSelector:\n    matchNames:\n      - production\n  selector:\n    matchLabels:\n      app: my-app\n  endpoints:\n  - path: /metrics\n</code></pre> <p>And here is an example of how to use a label selector that matches the labels on the service across all namespaces:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: my-service-monitor\n  namespace: monitoring\n  labels:\n    app: my-app\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n  endpoints:\n  - path: /metrics\n  namespaceSelector:\n    any: true\n</code></pre>"},{"location":"archive/prometheus/#pod-monitor","title":"Pod Monitor","text":"<ul> <li>Pod Monitor API Doc</li> </ul>"},{"location":"archive/prometheus/#exporters-and-integrations","title":"Exporters and integrations","text":"<ul> <li>Third Party Exporters</li> <li>Understanding and Building exporters</li> </ul>"},{"location":"archive/secret-management/","title":"Secret management","text":""},{"location":"archive/secret-management/#introduction","title":"Introduction","text":"<p>When deploying applications in a cluster, it is essential to manage secrets (API keys, passwords, tokens, etc.) securely. This ensures that sensitive data is protected from unauthorized access and minimizes the potential for security breaches.</p>"},{"location":"archive/secret-management/#challenges-of-secret-management","title":"Challenges of Secret Management","text":"<p>There are several challenges associated with secret management in a clustered environment:</p> <ul> <li> <p>Security: Storing sensitive data in plaintext or hardcoding it within applications can lead to security vulnerabilities.</p> </li> <li> <p>Scalability: As the number of services and applications increases, managing secrets becomes complex.</p> </li> <li> <p>Versioning and Synchronization: Ensuring that secrets are updated and synchronized across all instances of a service can be difficult.</p> </li> </ul>"},{"location":"archive/secret-management/#what-is-the-external-secrets-resource","title":"What is the External Secrets resource?","text":"<p>The External Secrets Operator provides the translation layer between Kubernetes native secrets and external secrets. By utilizing external secrets, applications can easily retrieve the required secrets without exposing them in plaintext or hardcoding them within the application</p>"},{"location":"archive/secret-management/#what-about-doppler","title":"What about Doppler?","text":"<p>Doppler provides an easy-to-use interface and API for managing secrets, and supports a wide range of integrations with different platforms.</p> <p>By using Doppler as the centralized secrets provider, you can store and manage secrets in one place, while allowing applications to securely access them through the External Secrets API.</p>"},{"location":"archive/secret-management/#set-up-doppler","title":"Set Up Doppler","text":"<ol> <li>Sign up for a Doppler account at https://dashboard.doppler.com/signup</li> <li> <p>Follow the documentation for the doppler provider until \"Use Cases\". The Service Token is located under the <code>kube-system/external-secrets/stores/doppler</code>. Instead of using a SecretStore a ClusterSecretStore is used because we want to retrieve secrets from all namespaces.</p> </li> <li> <p><code>doppler_cluster_store.yaml</code>: Defines a <code>ClusterSecretStore</code>. A <code>ClusterSecretStore</code> is a custom resource provided by the External Secrets Manager that defines the connection and authentication information needed to access an external secret management system i.e, Doppler API in this case. This resource acts as a centralized reference for other resources within the Kubernetes cluster when they need to access the secrets stored in the external system.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: doppler-auth-api\nspec:\n  provider:\n    doppler:\n      auth:\n        secretRef:\n          dopplerToken:\n            name: doppler-token-auth-api\n            key: dopplerToken\n</code></pre> </li> <li> <p><code>secret.sops.yaml</code>: The authentication credentials in plain yaml. The contents of the file are similar to :</p> <pre><code>apiVersion: v1\ndata:\n    dopplerToken: aG91c2Uud29ybGR4eHp3NkZBRzQ2Q3RIN1dXQUlKbTlJZTZOa05Jd1BzV1lqZjNzZ0JKc3oK\nkind: Secret\nmetadata:\n    name: doppler-token-auth-api\n</code></pre> </li> <li> <p><code>test.yaml</code>: Secret to test that the configuration has been done correctly.</p> </li> <li> <p>Create a new project and configure your secrets</p> </li> </ol>"},{"location":"archive/secret-management/#secret-configuration","title":"Secret Configuration","text":""},{"location":"archive/secret-management/#clustersecretstore-vs-secretstore","title":"ClusterSecretStore vs SecretStore","text":"<p>There are two types of secret stores provided by the External Secrets Manager:</p> <ul> <li><code>ClusterSecretStore:</code> A cluster-wide secret store that can be referenced by ExternalSecret resources across all namespaces in the cluster.</li> <li><code>SecretStore:</code> A namespace-scoped secret store that can only be referenced by ExternalSecret resources within the same namespace.</li> </ul>"},{"location":"archive/secret-management/#clustersecretstore","title":"ClusterSecretStore","text":"<p>A ClusterSecretStore is suitable for scenarios where a single external secret provider (such as Doppler) is used by multiple applications or services in different namespaces. It provides centralized configuration, consistency, reduced duplication, and access control.</p>"},{"location":"archive/secret-management/#example","title":"Example","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: doppler-cluster-wide-secret-store\nspec:\n  provider:\n    doppler:\n      auth:\n        secretRef:\n          dopplerToken:\n            name: doppler-token\n            key: dopplerToken\n            namespace: kube-system\n</code></pre>"},{"location":"archive/secret-management/#secretstore","title":"SecretStore","text":"<p>A <code>SecretStore</code> is suitable for scenarios where different namespaces need to use different external secret providers or configurations. It provides more granular control over secret provider configurations within each namespace.</p>"},{"location":"archive/secret-management/#example_1","title":"Example","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: doppler-namespace-scoped-secret-store\n  namespace: my-namespace\nspec:\n  provider:\n    doppler:\n      auth:\n        secretRef:\n          dopplerToken:\n            name: doppler-token\n            key: dopplerToken\n</code></pre>"},{"location":"archive/secret-management/#secret-example","title":"Secret Example","text":"<pre><code>---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: auth-api-elasticsearch-ingestion-credentials\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: doppler-auth-api\n  target:\n    name: es-ingestion-credentials\n\n  data:\n    - secretKey: ELASTICSEARCH__PASSWORD\n      remoteRef:\n        key: ELASTICSEARCH__PASSWORD\n\n    - secretKey: ELASTICSEARCH__NAME\n      remoteRef:\n        key: ELASTICSEARCH__NAME\n</code></pre>"},{"location":"archive/secret-management/#secret-variations-with-flux","title":"Secret Variations with Flux","text":""},{"location":"archive/secret-management/#create-the-secret","title":"Create the Secret","text":"<ol> <li>Create in Doppler the <code>AWESOME_SECRET</code> variable with the desired value</li> <li>Create an external secret resource</li> </ol> <pre><code>---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: auth-api-elasticsearch-ingestion-credentials\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: doppler-auth-api\n  target:\n    name: application-secret\n\n  data:\n    - secretKey: AWESOME_SECRET\n      remoteRef:\n        key: AWESOME_SECRET\n</code></pre>"},{"location":"archive/secret-management/#method-1-envfrom","title":"Method 1: <code>envFrom</code>","text":"<p>Use <code>envFrom</code> in a deployment or a Helm chart that supports the setting, this will pass all secret items from the secret into the containers environment.</p> <pre><code>envFrom:\n  - secretRef:\n      name: application-secret\n</code></pre> <p>Add an example</p>"},{"location":"archive/secret-management/#method-2-envvaluefrom","title":"Method 2: <code>env.valueFrom</code>","text":"<p>Similar to the above but it's possible with <code>env</code> to pick an item from a secret.</p> <pre><code>env:\n  - name: WAY_COOLER_ENV_VARIABLE\n    valueFrom:\n      secretKeyRef:\n        name: application-secret\n        key: AWESOME_SECRET\n</code></pre> <p>Add an example</p>"},{"location":"archive/secret-management/#method-3-specvaluesfrom","title":"Method 3: <code>spec.valuesFrom</code>","text":"<p>The Flux HelmRelease option <code>valuesFrom</code> can inject a secret item into the Helm values of a <code>HelmRelease</code>  * _Does not work with merging array values  * _Care needed with keys that contain dot notation in the name</p> <pre><code>valuesFrom:\n  - targetPath: config.\"admin\\.password\"\n    kind: Secret\n    name: application-secret\n    valuesKey: AWESOME_SECRET\n</code></pre> <p>Add an example</p>"},{"location":"archive/secret-management/#method-4-variable-substitution-with-flux","title":"Method 4: Variable Substitution with Flux","text":"<p>Flux variable substitution can inject secrets into any YAML manifest. This requires the Flux Kustomization configured to enable variable substitution. Correctly configured this allows you to use <code>${GLOBAL_AWESOME_SECRET}</code> in any YAML manifest.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: cluster-secrets\n  namespace: flux-system\nstringData:\n  GLOBAL_AWESOME_SECRET: \"GLOBAL SUPER SECRET VALUE\"\n</code></pre> <pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\n# ...\nspec:\n# ...\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n  postBuild:\n    substituteFrom:\n      - kind: Secret\n        name: cluster-secrets\n</code></pre> <p>Add an example</p> <ul> <li> <p>For the first three methods consider using a tool like stakater/reloader to restart the pod when the secret changes.</p> </li> <li> <p>Using reloader on a pod using a secret provided by Flux Variable Substitution will lead to pods being restarted during any change to the secret while related to the pod or not.</p> </li> <li> <p>The last method should be used when all other methods are not an option, or used when you have a \u201cglobal\u201d secret used by a bunch of YAML manifests.</p> </li> </ul>"},{"location":"archive/secret-management/#install-doppler-cli","title":"Install Doppler CLI","text":"<p>Follow the installation instructions for your operating system at https://docs.doppler.com/docs/enclave-installation</p>"},{"location":"archive/secret-management/#list-of-secrets","title":"List of secrets","text":"<pre><code>$ doppler secrets --only-names\n</code></pre>"},{"location":"archive/semantic_git_messages/","title":"Semmantic Commits","text":""},{"location":"archive/semantic_git_messages/#overview","title":"Overview","text":"<p>In this repo, I've tried to use semantic commit messages to make changes more descriptive and clear. This helps with readability and tracking changes, even in personal projects.</p> <p>A good commit message format follows the pattern: <code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;</code></p>"},{"location":"archive/semantic_git_messages/#example","title":"Example","text":"<pre><code>feat: add hat wobble\n^--^  ^------------^\n|     |\n|     +-&gt; Summary in present tense.\n|\n+-------&gt; Type: chore, docs, feat, fix, refactor, style, or test.\n</code></pre> <p>More examples</p> <ul> <li>fix(auth): resolve incorrect password validation</li> <li>docs: add installation guide to README</li> <li>feat: add search feature</li> <li>refactor: improve UserService API calls</li> <li>test: create user registration tests</li> </ul> <p>Key points for a good commit message:</p> <ol> <li>Use a clear and concise <code>&lt;description&gt;</code> that summarizes the change.</li> <li>Use the imperative mood for <code>&lt;description&gt;</code> (e.g., \"resolve\" instead of \"resolved\").</li> <li>Limit the <code>&lt;description&gt;</code> line to 72 characters or fewer.</li> <li><code>&lt;type&gt;</code> should be one of the following: fix, feat, docs, style, refactor, test, chore, ci, build, perf.</li> <li>Optionally, include a <code>&lt;scope&gt;</code> to provide more context about the affected part of the codebase.</li> <li>If the commit closes an issue, include \"Closes #issue-number\" in the description.</li> </ol> <p>Check out these resources:</p> <ul> <li>https://www.conventionalcommits.org/</li> <li>https://seesparkbox.com/foundry/semantic_commit_messages</li> <li>http://karma-runner.github.io/1.0/dev/git-commit-msg.html</li> <li>https://fluxcd.io/contributing/flux/#format-of-the-commit-message</li> </ul>"},{"location":"archive/troubleshooting/","title":"Troubleshooting","text":""},{"location":"archive/troubleshooting/#force-reconcile","title":"Force Reconcile","text":"<pre><code>flux reconcile -n flux-system source git flux-cluster\nflux reconcile -n flux-system kustomization flux-cluster\n</code></pre>"},{"location":"archive/troubleshooting/#troubleshooting-volume-with-multipath","title":"Troubleshooting volume with multipath","text":"<p>Issue: If you are getting pods that will not start due to timeout issues. You may also see the <code>share-manager-pvc-xxx</code> pods crashing and restarting over and over again. If this is your issue(s), look into this first.</p> <ul> <li>Official Longhorn Troubleshooting Documentation</li> </ul>"},{"location":"archive/troubleshooting/#renaming-network-interface","title":"Renaming Network Interface","text":"<p>Issue: If your network interface does not match your other nodes, you can change it. This is useful if you are using Ansible and expect all network interfaces to be the same.</p> <ul> <li>Change Network Interface Name</li> </ul>"},{"location":"archive/volumes-and-storage/","title":"Volumes and Storage","text":"<p>To ensure that your application's data is not lost, you will need to establish persistent storage when deploying it. By utilizing persistent storage, you can store the application data externally from the pod that runs the application. This storage approach allows you to retain the application data, even if the pod that the application runs on fails.</p> <p>In Kubernetes, a persistent volume (PV) is a storage unit within the cluster, while a persistent volume claim (PVC) is a request for storage. To learn more about the functionality of PVs and PVCs, you can consult the official Kubernetes documentation on storage.</p> <p>This page provides instructions on how to configure persistent storage using either a local storage provider or Longhorn.</p>"},{"location":"archive/volumes-and-storage/#databases","title":"Databases","text":"<pre><code>GRANT linkding TO admin;\nGRANT linkding TO postgres;\nSELECT rolname FROM pg_roles;\nSELECT usename FROM pg_user;\npostgres=&gt; GRANT gitea TO postgres;\nGRANT &lt;ROLE&gt; TO &lt;USER&gt;\n</code></pre>"},{"location":"archive/logs/","title":"Flow &amp; Policy Logging","text":"<p>Log streaming platforms are systems that efficiently gather, process, and analyze log data from a wide range of sources in near real-time. These platforms are specifically engineered to manage high volumes of log data, offering valuable insights into system and application performance.</p> <p>Below is an overview of the key phases and components involved in a log streaming platform:</p> <p></p>"},{"location":"archive/logs/#data-collection","title":"Data Collection","text":"<p>The platform collects log data from various sources such as containers running inside K8s or logs from JournalD. Vector from datadog is used, serving as both an agent for collecting logs and metrics and an aggregator for processing and forwarding data. Other valid options are filebeat of fluentd.</p>"},{"location":"archive/logs/#message-queuing","title":"Message Queuing","text":"<p>To ensure efficient and reliable log data transfer, we use a message queueing system. This approach decouples data producers and consumers, allowing for scalability, fault tolerance, and improved data processing.</p>"},{"location":"archive/logs/#hot-path-and-cold-path","title":"Hot Path and Cold Path","text":"<p>The log data collection process is divided into two paths: hot path and cold path.</p> <ul> <li> <p>Hot Path: Involves real-time data processing, analysis, and alerting. This path is designed for low-latency data access and quick response to potential issues.</p> </li> <li> <p>Cold Path: Deals with long-term storage, indexing, and batch processing of log data. It allows for historical analysis and trend identification, often used for optimization and capacity planning.</p> </li> </ul>"},{"location":"archive/logs/#log-data-storage-and-analysis","title":"Log Data Storage and Analysis","text":"<ul> <li> <p>Elasticsearch: A distributed, RESTful search and analytics engine for storing, searching, and analyzing log data.</p> </li> <li> <p>Loki: A horizontally scalable, highly available log aggregation system designed for simplicity and cost-efficiency, providing a streamlined querying and visualization experience in Grafana.</p> </li> </ul>"},{"location":"archive/logs/#visualization","title":"Visualization","text":"<ul> <li> <p>Kibana: A data visualization and exploration tool that integrates with Elasticsearch, providing an intuitive interface for exploring, visualizing, and managing the log data.</p> </li> <li> <p>Grafana: A powerful visualization for creating dashboards, allowing you to explore and analyze different data sources.</p> </li> </ul>"},{"location":"archive/logs/#services","title":"Services","text":"<ul> <li>Elasticsearch</li> </ul>"},{"location":"archive/logs/elk/","title":"Elastisearch","text":"<p>WIP</p>"},{"location":"archive/logs/elk/#overview","title":"Overview","text":""},{"location":"archive/logs/elk/#architecture","title":"Architecture","text":"<p>Elasticsearch is deployed using hot/warm architecture which is a common design pattern for storing and managing data in Elasticsearch. In this architecture, data is divided into two tiers: a hot tier for actively used data and a warm tier for less frequently used data.</p> <p>Here's how it works:</p> <ul> <li>Hot tier: The hot tier consists of Elasticsearch indices that store data that is actively used and updated on a regular basis. This data is usually indexed on fast storage, such as solid-state drives (SSDs), and it is queried and analyzed frequently.</li> <li>Warm tier: The warm tier consists of Elasticsearch indices that store data that is less frequently used and updated. This data is usually indexed on slower storage, such as hard disk drives (HDDs), and it is queried and analyzed less frequently than the data in the hot tier.</li> </ul> <p>The hot/warm architecture allows you to store and manage large amounts of data in Elasticsearch while minimizing the cost and maintenance overhead of the cluster. It is particularly useful for applications that generate large amounts of data and need to retain it for long periods of time.</p> <p>To implement the hot/warm architecture in Elasticsearch, is used</p> <ul> <li>index lifecycle management (ILM) to automatically move indices between the hot and warm tiers based on their age or usage patterns.</li> <li>index templates to define the settings and mapping of indices in the hot and warm tiers, and use search templates to route queries to the appropriate tier based on the data being accessed.</li> </ul> <p>Also the nodes that store data in the hot tier has the role of \"master\" and the nodes that store data in the warm tier has the \"data tier\".</p> <p>This setup can provide better performance and efficiency for the cluster, as the master nodes can focus on managing the cluster and processing requests for actively used data, while the data nodes can focus on storing and processing less frequently used data. This setup can provide better availability and fault tolerance, as the master nodes can continue to operate even if one or more data nodes fail.</p>"},{"location":"archive/logs/elk/#infrastructure","title":"Infrastructure","text":"<ul> <li>TBD Hot and Warm nodes</li> </ul>"},{"location":"archive/logs/elk/#administration","title":"Administration","text":""},{"location":"archive/logs/elk/#users","title":"Users","text":"<p>If you're using Kibana Dev Tools to create a user in Elasticsearch, you can use the Elasticsearch Security API directly within the Kibana Console. Here's an example of how to create a user using the PUT method with the <code>/_security/user</code> endpoint. Here is a link to the create or update users API documentation.</p> <pre><code>PUT /_security/user/ingestion\n{\n  \"password\" : \"something\",\n  \"roles\" : [ \"superuser\" ]\n}\n</code></pre>"},{"location":"archive/logs/elk/#index-management","title":"Index Management","text":"<p>Elasticsearch ILM policies are used to automatically manage the data streams according the performance, resiliency, and retention requirements.</p>"},{"location":"archive/logs/elk/#index-lifecycle-policy","title":"Index Lifecycle Policy","text":"<p>The following policies have been created.</p> Policy Name Rollover Policy Warm Phase Delete Index Template logs-k3s 200 Mb/5d 10d 15d logs"},{"location":"archive/logs/elk/#configure-a-lifecycle-policy","title":"Configure a lifecycle policy","text":""},{"location":"archive/logs/elk/#create-lifecycle-policy","title":"Create lifecycle policy","text":"<p>Elasticsearch ILM policy documentation</p> <pre><code>PUT _ilm/policy/logs-k3s\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"5d\",\n            \"max_primary_shard_size\": \"200mb\",\n            \"min_primary_shard_size\": \"200mb\"\n          },\n          \"set_priority\": {\n            \"priority\": 100\n          },\n          \"shrink\": {\n            \"number_of_shards\": 1\n          }\n        },\n        \"min_age\": \"0ms\"\n      },\n      \"warm\": {\n        \"min_age\": \"10d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 50\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"20d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"archive/logs/elk/#apply-lifecycle-policy-with-an-index-template","title":"Apply lifecycle policy with an index template","text":"<pre><code>PUT _index_template/logs-k3s\n{\n  \"priority\": 101,\n  \"template\": {\n    \"settings\": {\n      \"index\": {\n        \"lifecycle\": {\n          \"name\": \"logs-k3s\"\n        },\n        \"number_of_shards\": \"1\",\n        \"number_of_replicas\": \"0\"\n      }\n    }\n  },\n  \"index_patterns\": [\n    \"logs-k3s_*\"\n  ],\n  \"data_stream\": {},\n  \"composed_of\": [\n    \"logs-mappings\",\n    \"data-streams-mappings\",\n    \"logs-settings\"\n  ]\n}\n</code></pre> <ol> <li>Use this template for all new indices whose names begin with <code>logs-k3s_*\"</code></li> <li>Apply <code>logs-k3s</code> to new indices created with this template</li> </ol>"},{"location":"archive/logs/elk/#index-and-component-templates","title":"Index and Component templates","text":"<p>The logs is composed by the following Component Templates:</p> <ul> <li>logs-mappings</li> <li>data-streams-mappings</li> <li>logs-settings</li> </ul>"},{"location":"archive/logs/elk/#snapshot-and-restore","title":"Snapshot and restore","text":""},{"location":"archive/logs/elk/#monitoring","title":"Monitoring","text":"<ul> <li>Search and indexing performance</li> <li>Memory and garbage collection</li> <li>Host-level system and network metrics</li> <li>Cluster health and node availability</li> <li>Resource saturation and errors</li> </ul>"},{"location":"archive/logs/elk/#cluster-health","title":"Cluster health","text":"METRIC DESCRIPTION ELASTICSEARCH EXPORTER METRIC NAME Cluster status <code>elasticsearch_cluster_health_status</code> Number of unasigned shards <code>elasticsearch_cluster_health_unassigned_shards</code>"},{"location":"archive/logs/elk/#search-performance","title":"Search Performance","text":"Metric description Name Metric type Total number of queries <code>elasticsearch_indices_search_query_total</code> Work: Throughput Total time spent on queries <code>elasticsearch_indices_search_query_time_seconds</code> Work: Performance Number of queries currently in progress ** <code>indices.search.query_current</code> Work: Throughput Total number of fetches <code>elasticsearch_indices_search_fetch_total</code> Work: Throughput Total time spent on fetches <code>elasticsearch_indices_search_fetch_time_seconds</code> Work: Performance Number of fetches currently in progress  ** <code>indices.search.fetch_current</code> Work: Throughput"},{"location":"archive/logs/elk/#alerting","title":"Alerting","text":"<p>TBD</p>"},{"location":"archive/logs/elk/#references","title":"References","text":"<ul> <li> <p>Elasticsearch documentation</p> </li> <li> <p>Top 10 Elasticsearch Metrics to Watch</p> </li> <li> <p>Elastic Stack Configuration policies</p> </li> <li> <p>Elastic Init containers</p> </li> <li> <p>Init container</p> </li> </ul>"},{"location":"archive/metrics/","title":"Ops &amp; Sec Metrics","text":"<p>Prometheus metrics are collected from Kubernetes clusters and use industry standard tools for analyzing and alerting such as PromQL and Grafana</p> <p> Source: Prometheus Stack Review</p>"},{"location":"archive/metrics/#services","title":"Services","text":"<ul> <li>Prometheus</li> </ul>"},{"location":"archive/metrics/prometheus/","title":"Prometheus","text":""},{"location":"archive/metrics/prometheus/#introduction","title":"Introduction","text":"<p>This document explains the metrics ingestion pipeline in a Kubernetes cluster using the Prometheus Operator and its related custom resources, ServiceMonitors and PodMonitors.</p>"},{"location":"archive/metrics/prometheus/#deployment","title":"Deployment","text":"<p>TBD</p>"},{"location":"archive/metrics/prometheus/#metrics-ingestion","title":"Metrics Ingestion","text":""},{"location":"archive/metrics/prometheus/#scrape-frequency","title":"Scrape frequency","text":"<p>The default scrape frequency for all default targets and scrapes is 60s</p>"},{"location":"archive/metrics/prometheus/#default-targets-scraped","title":"Default Targets scraped","text":"<p>These jobs involve scraping metrics from the core components of the k3s cluster. There are additional jobs that haven't been listed here, as they aren't deemed critical to the cluster's overall functionality.</p> <p>To retrieve the available jobs in the cluster, you can use the following query:</p> <pre><code>sum by (job) (up)\n</code></pre> <ul> <li> <p><code>apiserver</code> (<code>{job=\"apiserver\"}</code>)</p> </li> <li> <p><code>kube-controller-manager</code> (<code>{job=\"kube-controller-manager\"}</code>)</p> </li> <li> <p><code>kube-scheduler</code> (<code>{job=\"kube-scheduler\"}</code>)</p> </li> <li> <p><code>kubelet</code> (<code>{job=\"kubelet\"}</code>)</p> </li> <li> <p><code>kube-state-metrics</code> (<code>{job=\"kube-state-metrics\"}</code>)</p> </li> <li> <p><code>coredns</code> (<code>{job=\"coredns\"}</code>)</p> </li> <li> <p><code>prometheus-alertmanager</code> (<code>{job=\"prometheus-alertmanager\"}</code>)</p> </li> <li> <p><code>prometheus-operator</code> (<code>{job=\"prometheus-operator\"}</code>)</p> </li> <li> <p><code>prometheus</code> (<code>{job=\"prometheus-prometheus\"}</code>)</p> </li> </ul>"},{"location":"archive/metrics/prometheus/#metrics-collected-from-targets","title":"Metrics collected from targets","text":"<p>The metrics gathered by default from each standard target are listed here, while all other metrics are discarded using relabeling rules. To obtain the metrics from these standard targets, for example, <code>job=\"kubelet\"</code> you can execute the following:</p> <pre><code>group by (__name__) ({job=\"kubelet\"})\n</code></pre> apiserver (click to expand) <ul> <li><code>apiserver_admission_controller_admission_duration_seconds_bucket</code></li> <li><code>apiserver_admission_controller_admission_duration_seconds_count</code></li> <li><code>apiserver_admission_controller_admission_duration_seconds_sum</code></li> <li><code>apiserver_admission_step_admission_duration_seconds_bucket</code></li> <li><code>apiserver_admission_step_admission_duration_seconds_count</code></li> <li><code>apiserver_admission_step_admission_duration_seconds_sum</code></li> <li><code>apiserver_admission_step_admission_duration_seconds_summary</code></li> <li><code>apiserver_admission_step_admission_duration_seconds_summary_count</code></li> <li><code>apiserver_admission_step_admission_duration_seconds_summary_sum</code></li> <li><code>apiserver_admission_webhook_admission_duration_seconds_bucket</code></li> <li><code>apiserver_admission_webhook_admission_duration_seconds_count</code></li> <li><code>apiserver_admission_webhook_admission_duration_seconds_sum</code></li> <li><code>apiserver_admission_webhook_fail_open_count</code></li> <li><code>apiserver_admission_webhook_rejection_count</code></li> <li><code>apiserver_admission_webhook_request_total</code></li> <li><code>apiserver_audit_event_total</code></li> <li><code>apiserver_audit_requests_rejected_total</code></li> <li><code>apiserver_current_inflight_requests</code></li> <li><code>apiserver_current_inqueue_requests</code></li> <li><code>apiserver_delegated_authn_request_duration_seconds_bucket</code></li> <li><code>apiserver_delegated_authn_request_duration_seconds_count</code></li> <li><code>apiserver_delegated_authn_request_duration_seconds_sum</code></li> <li><code>apiserver_delegated_authn_request_total</code></li> <li><code>apiserver_delegated_authz_request_duration_seconds_bucket</code></li> <li><code>apiserver_delegated_authz_request_duration_seconds_count</code></li> <li><code>apiserver_delegated_authz_request_duration_seconds_sum</code></li> <li><code>apiserver_delegated_authz_request_total</code></li> <li><code>apiserver_init_events_total</code></li> <li><code>apiserver_longrunning_requests</code></li> <li><code>apiserver_request_aborts_total</code></li> <li><code>apiserver_request_duration_seconds_count</code></li> <li><code>apiserver_request_duration_seconds_sum</code></li> <li><code>apiserver_request_filter_duration_seconds_bucket</code></li> <li><code>apiserver_request_filter_duration_seconds_count</code></li> <li><code>apiserver_request_filter_duration_seconds_sum</code></li> <li><code>apiserver_request_post_timeout_total</code></li> <li><code>apiserver_request_sli_duration_seconds_count</code></li> <li><code>apiserver_request_sli_duration_seconds_sum</code></li> <li><code>apiserver_request_slo_duration_seconds_count</code></li> <li><code>apiserver_request_slo_duration_seconds_sum</code></li> <li><code>apiserver_request_terminations_total</code></li> <li><code>apiserver_request_timestamp_comparison_time_bucket</code></li> <li><code>apiserver_request_timestamp_comparison_time_count</code></li> <li><code>apiserver_request_timestamp_comparison_time_sum</code></li> <li><code>apiserver_request_total</code></li> <li><code>apiserver_requested_deprecated_apis</code></li> <li><code>apiserver_response_sizes_count</code></li> <li><code>apiserver_response_sizes_sum</code></li> <li><code>apiserver_selfrequest_total</code></li> <li><code>apiserver_storage_db_total_size_in_bytes</code></li> <li><code>apiserver_storage_list_evaluated_objects_total</code></li> <li><code>apiserver_storage_list_fetched_objects_total</code></li> <li><code>apiserver_storage_list_returned_objects_total</code></li> <li><code>apiserver_storage_list_total</code></li> <li><code>apiserver_storage_objects</code></li> <li><code>apiserver_tls_handshake_errors_total</code></li> <li><code>apiserver_watch_events_sizes_count</code></li> <li><code>apiserver_watch_events_sizes_sum</code></li> <li><code>apiserver_watch_events_total</code></li> <li><code>count:up</code></li> <li><code>scrape_duration_seconds</code></li> <li><code>scrape_samples_post_metric_relabeling</code></li> <li><code>scrape_samples_scraped</code></li> <li><code>scrape_series_added</code></li> <li><code>up</code></li> </ul> kube-controller-manager\" (click to expand) <ul> <li><code>apiserver_audit_event_total</code></li> <li><code>apiserver_audit_requests_rejected_total</code></li> <li><code>apiserver_client_certificate_expiration_seconds_bucket</code></li> <li><code>apiserver_client_certificate_expiration_seconds_count</code></li> <li><code>apiserver_client_certificate_expiration_seconds_sum</code></li> <li><code>apiserver_delegated_authn_request_duration_seconds_bucket</code></li> <li><code>apiserver_delegated_authn_request_duration_seconds_count</code></li> <li><code>apiserver_delegated_authn_request_duration_seconds_sum</code></li> <li><code>apiserver_delegated_authn_request_total</code></li> <li><code>apiserver_delegated_authz_request_duration_seconds_bucket</code></li> <li><code>apiserver_delegated_authz_request_duration_seconds_count</code></li> <li><code>apiserver_delegated_authz_request_duration_seconds_sum</code></li> <li><code>apiserver_delegated_authz_request_total</code></li> <li><code>apiserver_envelope_encryption_dek_cache_fill_percent</code></li> <li><code>apiserver_storage_data_key_generation_duration_seconds_bucket</code></li> <li><code>apiserver_storage_data_key_generation_duration_seconds_count</code></li> <li><code>apiserver_storage_data_key_generation_duration_seconds_sum</code></li> <li><code>apiserver_storage_data_key_generation_failures_total</code></li> <li><code>apiserver_storage_db_total_size_in_bytes</code></li> <li><code>apiserver_storage_envelope_transformation_cache_misses_total</code></li> <li><code>apiserver_storage_list_evaluated_objects_total</code></li> <li><code>apiserver_storage_list_fetched_objects_total</code></li> <li><code>apiserver_storage_list_returned_objects_total</code></li> <li><code>apiserver_storage_list_total</code></li> <li><code>apiserver_storage_objects</code></li> <li><code>apiserver_webhooks_x509_insecure_sha_total</code></li> <li><code>apiserver_webhooks_x509_missing_san_total</code></li> <li><code>attachdetach_controller_forced_detaches</code></li> <li><code>attachdetach_controller_total_volumes</code></li> <li><code>authenticated_user_requests</code></li> <li><code>authentication_attempts</code></li> <li><code>authentication_duration_seconds_bucket</code></li> <li><code>authentication_duration_seconds_count</code></li> <li><code>authentication_duration_seconds_sum</code></li> <li><code>authentication_token_cache_active_fetch_count</code></li> <li><code>authentication_token_cache_fetch_total</code></li> <li><code>authentication_token_cache_request_duration_seconds_bucket</code></li> <li><code>authentication_token_cache_request_duration_seconds_count</code></li> <li><code>authentication_token_cache_request_duration_seconds_sum</code></li> <li><code>authentication_token_cache_request_total</code></li> <li><code>count:up</code></li> <li><code>cronjob_controller_job_creation_skew_duration_seconds_bucket</code></li> <li><code>cronjob_controller_job_creation_skew_duration_seconds_count</code></li> <li><code>cronjob_controller_job_creation_skew_duration_seconds_sum</code></li> <li><code>disabled_metric_total</code></li> <li><code>endpoint_slice_controller_changes</code></li> <li><code>endpoint_slice_controller_desired_endpoint_slices</code></li> <li><code>endpoint_slice_controller_endpoints_added_per_sync_bucket</code></li> <li><code>endpoint_slice_controller_endpoints_added_per_sync_count</code></li> <li><code>endpoint_slice_controller_endpoints_added_per_sync_sum</code></li> <li><code>endpoint_slice_controller_endpoints_desired</code></li> <li><code>endpoint_slice_controller_endpoints_removed_per_sync_bucket</code></li> <li><code>endpoint_slice_controller_endpoints_removed_per_sync_count</code></li> <li><code>endpoint_slice_controller_endpoints_removed_per_sync_sum</code></li> <li><code>endpoint_slice_controller_endpointslices_changed_per_sync_bucket</code></li> <li><code>endpoint_slice_controller_endpointslices_changed_per_sync_count</code></li> <li><code>endpoint_slice_controller_endpointslices_changed_per_sync_sum</code></li> <li><code>endpoint_slice_controller_num_endpoint_slices</code></li> <li><code>endpoint_slice_controller_syncs</code></li> <li><code>endpoint_slice_mirroring_controller_addresses_skipped_per_sync_bucket</code></li> <li><code>endpoint_slice_mirroring_controller_addresses_skipped_per_sync_count</code></li> <li><code>endpoint_slice_mirroring_controller_addresses_skipped_per_sync_sum</code></li> <li><code>endpoint_slice_mirroring_controller_changes</code></li> <li><code>endpoint_slice_mirroring_controller_desired_endpoint_slices</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_added_per_sync_bucket</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_added_per_sync_count</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_added_per_sync_sum</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_desired</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_removed_per_sync_bucket</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_removed_per_sync_count</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_removed_per_sync_sum</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_sync_duration_bucket</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_sync_duration_count</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_sync_duration_sum</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_updated_per_sync_bucket</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_updated_per_sync_count</code></li> <li><code>endpoint_slice_mirroring_controller_endpoints_updated_per_sync_sum</code></li> <li><code>endpoint_slice_mirroring_controller_num_endpoint_slices</code></li> <li><code>ephemeral_volume_controller_create_failures_total</code></li> <li><code>ephemeral_volume_controller_create_total</code></li> <li><code>garbagecollector_controller_resources_sync_error_total</code></li> <li><code>go_cgo_go_to_c_calls_calls_total</code></li> <li><code>go_gc_cycles_automatic_gc_cycles_total</code></li> <li><code>go_gc_cycles_forced_gc_cycles_total</code></li> <li><code>go_gc_cycles_total_gc_cycles_total</code></li> <li><code>go_gc_duration_seconds</code></li> <li><code>go_gc_duration_seconds_count</code></li> <li><code>go_gc_duration_seconds_sum</code></li> <li><code>go_gc_heap_allocs_by_size_bytes_bucket</code></li> <li><code>go_gc_heap_allocs_by_size_bytes_count</code></li> <li><code>go_gc_heap_allocs_by_size_bytes_sum</code></li> <li><code>go_gc_heap_allocs_bytes_total</code></li> <li><code>go_gc_heap_allocs_objects_total</code></li> <li><code>go_gc_heap_frees_by_size_bytes_bucket</code></li> <li><code>go_gc_heap_frees_by_size_bytes_count</code></li> <li><code>go_gc_heap_frees_by_size_bytes_sum</code></li> <li><code>go_gc_heap_frees_bytes_total</code></li> <li><code>go_gc_heap_frees_objects_total</code></li> <li><code>go_gc_heap_goal_bytes</code></li> <li><code>go_gc_heap_objects_objects</code></li> <li><code>go_gc_heap_tiny_allocs_objects_total</code></li> <li><code>go_gc_limiter_last_enabled_gc_cycle</code></li> <li><code>go_gc_pauses_seconds_bucket</code></li> <li><code>go_gc_pauses_seconds_count</code></li> <li><code>go_gc_pauses_seconds_sum</code></li> <li><code>go_gc_stack_starting_size_bytes</code></li> <li><code>go_goroutines</code></li> <li><code>go_info</code></li> <li><code>go_memory_classes_heap_free_bytes</code></li> <li><code>go_memory_classes_heap_objects_bytes</code></li> <li><code>go_memory_classes_heap_released_bytes</code></li> <li><code>go_memory_classes_heap_stacks_bytes</code></li> <li><code>go_memory_classes_heap_unused_bytes</code></li> <li><code>go_memory_classes_metadata_mcache_free_bytes</code></li> <li><code>go_memory_classes_metadata_mcache_inuse_bytes</code></li> <li><code>go_memory_classes_metadata_mspan_free_bytes</code></li> <li><code>go_memory_classes_metadata_mspan_inuse_bytes</code></li> <li><code>go_memory_classes_metadata_other_bytes</code></li> <li><code>go_memory_classes_os_stacks_bytes</code></li> <li><code>go_memory_classes_other_bytes</code></li> <li><code>go_memory_classes_profiling_buckets_bytes</code></li> <li><code>go_memory_classes_total_bytes</code></li> <li><code>go_memstats_alloc_bytes</code></li> <li><code>go_memstats_alloc_bytes_total</code></li> <li><code>go_memstats_buck_hash_sys_bytes</code></li> <li><code>go_memstats_frees_total</code></li> <li><code>go_memstats_gc_sys_bytes</code></li> <li><code>go_memstats_heap_alloc_bytes</code></li> <li><code>go_memstats_heap_idle_bytes</code></li> <li><code>go_memstats_heap_inuse_bytes</code></li> <li><code>go_memstats_heap_objects</code></li> <li><code>go_memstats_heap_released_bytes</code></li> <li><code>go_memstats_heap_sys_bytes</code></li> <li><code>go_memstats_last_gc_time_seconds</code></li> <li><code>go_memstats_lookups_total</code></li> <li><code>go_memstats_mallocs_total</code></li> <li><code>go_memstats_mcache_inuse_bytes</code></li> <li><code>go_memstats_mcache_sys_bytes</code></li> <li><code>go_memstats_mspan_inuse_bytes</code></li> <li><code>go_memstats_mspan_sys_bytes</code></li> <li><code>go_memstats_next_gc_bytes</code></li> <li><code>go_memstats_other_sys_bytes</code></li> <li><code>go_memstats_stack_inuse_bytes</code></li> <li><code>go_memstats_stack_sys_bytes</code></li> <li><code>go_memstats_sys_bytes</code></li> <li><code>go_sched_gomaxprocs_threads</code></li> <li><code>go_sched_goroutines_goroutines</code></li> <li><code>go_sched_latencies_seconds_bucket</code></li> <li><code>go_sched_latencies_seconds_count</code></li> <li><code>go_sched_latencies_seconds_sum</code></li> <li><code>go_threads</code></li> <li><code>hidden_metric_total</code></li> <li><code>job_controller_job_pods_finished_total</code></li> <li><code>job_controller_job_sync_duration_seconds_bucket</code></li> <li><code>job_controller_job_sync_duration_seconds_count</code></li> <li><code>job_controller_job_sync_duration_seconds_sum</code></li> <li><code>job_controller_job_syncs_total</code></li> <li><code>job_controller_jobs_finished_total</code></li> <li><code>job_controller_terminated_pods_tracking_finalizer_total</code></li> <li><code>kubernetes_build_info</code></li> <li><code>kubernetes_feature_enabled</code></li> <li><code>node_collector_evictions_total</code></li> <li><code>node_collector_unhealthy_nodes_in_zone</code></li> <li><code>node_collector_zone_health</code></li> <li><code>node_collector_zone_size</code></li> <li><code>node_ipam_controller_cidrset_allocation_tries_per_request_bucket</code></li> <li><code>node_ipam_controller_cidrset_allocation_tries_per_request_count</code></li> <li><code>node_ipam_controller_cidrset_allocation_tries_per_request_sum</code></li> <li><code>node_ipam_controller_cidrset_cidrs_allocations_total</code></li> <li><code>node_ipam_controller_cidrset_usage_cidrs</code></li> <li><code>process_cpu_seconds_total</code></li> <li><code>process_max_fds</code></li> <li><code>process_open_fds</code></li> <li><code>process_resident_memory_bytes</code></li> <li><code>process_start_time_seconds</code></li> <li><code>process_virtual_memory_bytes</code></li> <li><code>process_virtual_memory_max_bytes</code></li> <li><code>pv_collector_bound_pv_count</code></li> <li><code>pv_collector_bound_pvc_count</code></li> <li><code>pv_collector_total_pv_count</code></li> <li><code>registered_metric_total</code></li> <li><code>replicaset_controller_sorting_deletion_age_ratio_bucket</code></li> <li><code>replicaset_controller_sorting_deletion_age_ratio_count</code></li> <li><code>replicaset_controller_sorting_deletion_age_ratio_sum</code></li> <li><code>rest_client_exec_plugin_certificate_rotation_age_bucket</code></li> <li><code>rest_client_exec_plugin_certificate_rotation_age_count</code></li> <li><code>rest_client_exec_plugin_certificate_rotation_age_sum</code></li> <li><code>rest_client_exec_plugin_ttl_seconds</code></li> <li><code>rest_client_rate_limiter_duration_seconds_bucket</code></li> <li><code>rest_client_rate_limiter_duration_seconds_count</code></li> <li><code>rest_client_rate_limiter_duration_seconds_sum</code></li> <li><code>rest_client_request_duration_seconds_count</code></li> <li><code>rest_client_request_duration_seconds_sum</code></li> <li><code>rest_client_request_size_bytes_bucket</code></li> <li><code>rest_client_request_size_bytes_count</code></li> <li><code>rest_client_request_size_bytes_sum</code></li> <li><code>rest_client_requests_total</code></li> <li><code>rest_client_response_size_bytes_bucket</code></li> <li><code>rest_client_response_size_bytes_count</code></li> <li><code>rest_client_response_size_bytes_sum</code></li> <li><code>retroactive_storageclass_errors_total</code></li> <li><code>retroactive_storageclass_total</code></li> <li><code>root_ca_cert_publisher_sync_duration_seconds_bucket</code></li> <li><code>root_ca_cert_publisher_sync_duration_seconds_count</code></li> <li><code>root_ca_cert_publisher_sync_duration_seconds_sum</code></li> <li><code>root_ca_cert_publisher_sync_total</code></li> <li><code>running_managed_controllers</code></li> <li><code>scrape_duration_seconds</code></li> <li><code>scrape_samples_post_metric_relabeling</code></li> <li><code>scrape_samples_scraped</code></li> <li><code>scrape_series_added</code></li> <li><code>storage_count_attachable_volumes_in_use</code></li> <li><code>storage_operation_duration_seconds_bucket</code></li> <li><code>storage_operation_duration_seconds_count</code></li> <li><code>storage_operation_duration_seconds_sum</code></li> <li><code>ttl_after_finished_controller_job_deletion_duration_seconds_bucket</code></li> <li><code>ttl_after_finished_controller_job_deletion_duration_seconds_count</code></li> <li><code>ttl_after_finished_controller_job_deletion_duration_seconds_sum</code></li> <li><code>up</code></li> <li><code>volume_operation_total_seconds_bucket</code></li> <li><code>volume_operation_total_seconds_count</code></li> <li><code>volume_operation_total_seconds_sum</code></li> <li><code>workqueue_adds_total</code></li> <li><code>workqueue_depth</code></li> <li><code>workqueue_longest_running_processor_seconds</code></li> <li><code>workqueue_queue_duration_seconds_bucket</code></li> <li><code>workqueue_queue_duration_seconds_count</code></li> <li><code>workqueue_queue_duration_seconds_sum</code></li> <li><code>workqueue_retries_total</code></li> <li><code>workqueue_unfinished_work_seconds</code></li> <li><code>workqueue_work_duration_seconds_bucket</code></li> <li><code>workqueue_work_duration_seconds_count</code></li> <li><code>workqueue_work_duration_seconds_sum</code></li> </ul> cAdvisor (click to expand) <ul> <li><code>container_cpu_cfs_periods_total</code></li> <li><code>container_cpu_cfs_throttled_periods_total</code></li> <li><code>container_cpu_cfs_throttled_seconds_total</code></li> <li><code>container_cpu_usage_seconds_total</code></li> <li><code>container_memory_rss</code></li> <li><code>container_memory_usage_bytes</code></li> <li><code>container_memory_working_set_bytes</code></li> <li><code>container_network_receive_bytes_total</code></li> <li><code>container_network_transmit_bytes_total</code></li> <li><code>container_network_receive_packets_total</code></li> <li><code>container_network_transmit_packets_total</code></li> <li><code>container_network_receive_packets_dropped_total</code></li> <li><code>container_network_transmit_packets_dropped_total</code></li> <li><code>container_oom_events_total</code></li> <li><code>container_fs_reads_total</code></li> <li><code>container_fs_writes_total</code></li> <li><code>container_fs_reads_bytes_total</code></li> <li><code>container_fs_writes_bytes_total</code></li> </ul> <p>Check the following table in the cAdvisor documentation in case there is any doubt regards the type of the metric, what it is his purpose or the units</p> kubelet (click to expand) <ul> <li><code>kubelet_node_name</code></li> <li><code>kubelet_running_pods</code></li> <li><code>kubelet_running_pod_count</code></li> <li><code>kubelet_running_sum_containers</code></li> <li><code>kubelet_running_container_count</code></li> <li><code>volume_manager_total_volumes</code></li> <li><code>kubelet_node_config_error</code></li> <li><code>kubelet_runtime_operations_total</code></li> <li><code>kubelet_runtime_operations_errors_total</code></li> <li><code>kubelet_runtime_operations_duration_seconds_bucket</code></li> <li><code>kubelet_runtime_operations_duration_seconds_sum</code></li> <li><code>kubelet_runtime_operations_duration_seconds_count</code></li> <li><code>kubelet_pod_start_duration_seconds_bucket</code></li> <li><code>kubelet_pod_start_duration_seconds_sum</code></li> <li><code>kubelet_pod_start_duration_seconds_count</code></li> <li><code>kubelet_pod_worker_duration_seconds_bucket</code></li> <li><code>kubelet_pod_worker_duration_seconds_sum</code></li> <li><code>kubelet_pod_worker_duration_seconds_count</code></li> <li><code>storage_operation_duration_seconds_bucket</code></li> <li><code>storage_operation_duration_seconds_sum</code></li> <li><code>storage_operation_duration_seconds_count</code></li> <li><code>storage_operation_errors_total</code></li> <li><code>kubelet_cgroup_manager_duration_seconds_bucket</code></li> <li><code>kubelet_cgroup_manager_duration_seconds_sum</code></li> <li><code>kubelet_cgroup_manager_duration_seconds_count</code></li> <li><code>kubelet_pleg_relist_interval_seconds_bucket</code></li> <li><code>kubelet_pleg_relist_interval_seconds_count</code></li> <li><code>kubelet_pleg_relist_interval_seconds_sum</code></li> <li><code>kubelet_pleg_relist_duration_seconds_bucket</code></li> <li><code>kubelet_pleg_relist_duration_seconds_count</code></li> <li><code>kubelet_pleg_relist_duration_seconds_sum</code></li> <li><code>rest_client_requests_total</code></li> <li><code>rest_client_request_duration_seconds_bucket</code></li> <li><code>rest_client_request_duration_seconds_sum</code></li> <li><code>rest_client_request_duration_seconds_count</code></li> <li><code>process_resident_memory_bytes</code></li> <li><code>process_cpu_seconds_total</code></li> <li><code>go_goroutines</code></li> <li><code>kubernetes_build_info</code></li> </ul> node-exporter <ul> <li><code>node_memory_MemTotal_bytes</code></li> <li><code>node_cpu_seconds_total</code></li> <li><code>node_memory_MemAvailable_bytes</code></li> <li><code>node_memory_Buffers_bytes</code></li> <li><code>node_memory_Cached_bytes</code></li> <li><code>node_memory_MemFree_bytes</code></li> <li><code>node_memory_Slab_bytes</code></li> <li><code>node_filesystem_avail_bytes</code></li> <li><code>node_filesystem_size_bytes</code></li> <li><code>node_time_seconds</code></li> <li><code>node_exporter_build_info</code></li> <li><code>node_load1</code></li> <li><code>node_vmstat_pgmajfault</code></li> <li><code>node_network_receive_bytes_total</code></li> <li><code>node_network_transmit_bytes_total</code></li> <li><code>node_network_receive_drop_total</code></li> <li><code>node_network_transmit_drop_total</code></li> <li><code>node_disk_io_time_seconds_total</code></li> <li><code>node_disk_io_time_weighted_seconds_total</code></li> <li><code>node_load5</code></li> <li><code>node_load15</code></li> <li><code>node_disk_read_bytes_total</code></li> <li><code>node_disk_written_bytes_total</code></li> <li><code>node_uname_info</code></li> </ul> <p>Check the following documentation of prometheus node exporter in splunk</p> kube-state-metrics <ul> <li><code>kube_pod_owner</code></li> <li><code>kube_pod_container_resource_requests</code></li> <li><code>kube_pod_status_phase</code></li> <li><code>kube_pod_container_resource_limits</code></li> <li><code>kube_pod_info</code></li> <li><code>kube_pod_labels</code></li> <li><code>kube_pod_container_info</code></li> <li><code>kube_pod_container_status_waiting</code></li> <li><code>kube_pod_container_status_waiting_reason</code></li> <li><code>kube_pod_container_status_running</code></li> <li><code>kube_pod_container_status_terminated</code></li> <li><code>kube_pod_container_status_terminated_reason</code></li> <li><code>kube_pod_container_status_restarts_total</code></li> <li><code>kube_replicaset_owner</code></li> <li><code>kube_resourcequota</code></li> <li><code>kube_namespace_created</code></li> <li><code>kube_namespace_status_phase</code></li> <li><code>kube_node_spec_unschedulable</code></li> <li><code>kube_node_status_allocatable</code></li> <li><code>kube_node_status_capacity</code></li> <li><code>kube_node_info</code></li> <li><code>kube_node_status_condition</code></li> <li><code>kube_node_spec_taint</code></li> <li><code>kube_daemonset_created</code></li> <li><code>kube_daemonset_status_current_number_scheduled</code></li> <li><code>kube_daemonset_status_desired_number_scheduled</code></li> <li><code>kube_daemonset_status_number_available</code></li> <li><code>kube_daemonset_status_number_ready</code></li> <li><code>kube_daemonset_status_number_unavailable</code></li> <li><code>kube_daemonset_labels</code></li> <li><code>kube_deployment_labels</code></li> <li><code>kube_deployment_spec_replicas</code></li> <li><code>kube_deployment_status_replicas_available</code></li> <li><code>kube_deployment_status_replicas_unavailable</code></li> <li><code>kube_deployment_status_replicas_updated</code></li> <li><code>kube_statefulset_labels</code></li> <li><code>kube_statefulset_status_replicas_available</code></li> <li><code>kube_statefulset_status_replicas</code></li> <li><code>kube_statefulset_status_replicas_current</code></li> <li><code>kube_job_status_start_time</code></li> <li><code>kube_job_status_active</code></li> <li><code>kube_job_failed</code></li> <li><code>kube_horizontalpodautoscaler_status_desired_replicas</code></li> <li><code>kube_horizontalpodautoscaler_status_current_replicas</code></li> <li><code>kube_horizontalpodautoscaler_spec_min_replicas</code></li> <li><code>kube_horizontalpodautoscaler_spec_max_replicas</code></li> <li><code>kubernetes_build_info</code></li> </ul>"},{"location":"archive/metrics/prometheus/#k3s-services-monitoring","title":"k3s services monitoring","text":"<p>The kubernetes Documentation on System Metrics outlines the components that expose metrics in Prometheus format:</p> <ul> <li>kube-controller-manager (TCP 10257 metrics endpoint)</li> <li>kube-proxy (TCP 10249 /metrics endpoint)</li> <li>kube-apiserver (TCP 6443 /metrics at Kubernetes API port)</li> <li>kube-scheduler (TCP 10259 /metrics endpoint)</li> <li>kubelet (TCP 10250 /metrics, /metrics/cadvisor, /metrics/resource, and /metrics/probes endpoints)</li> </ul> <p>Info</p> <p>TCP port numbers for kube-scheduler and kube-controller-manager changed in Kubernetes release 1.22 (from 10251/10252 to 10257/10259). Additionally, an HTTPS authenticated connection is now required, so a Kubernetes authorized service account is needed to access the metrics service. Only the kube-proxy endpoint still uses HTTP; the rest now use HTTPS.</p> <p>Tip</p> <p>By default, K3S components (Scheduler, Controller Manager, and Proxy) do not expose their endpoints for metric collection. Their /metrics endpoints are bound to 127.0.0.1, only exposing them to localhost and preventing remote queries. To change this behavior, provide the following K3S installation arguments:</p> <pre><code>--kube-controller-manager-arg 'bind-address=0.0.0.0'\n--kube-proxy-arg 'metrics-bind-address=0.0.0.0'\n--kube-scheduler-arg 'bind-address=0.0.0.0'\n</code></pre> <p>kube-prometheus-stack creates the kubernetes resources needed to scrape metrics from all k8S components in a standard Kubernetes distribution, but these objects are not valid for a K3S cluster. K3S has a unique behavior related to metrics exposure. It deploys one process on each cluster node: <code>k3s-server</code> on master nodes or <code>k3s-agent</code> on worker nodes.</p> <p>All Kubernetes components running on a node share the same memory, and k3s emits the same metrics at all <code>/metrics</code> endpoints available on a node: <code>api-server</code>, <code>kubelet</code> (<code>TCP 10250</code>), <code>kube-proxy</code> (<code>TCP 10249</code>), <code>kube-scheduler</code> (<code>TCP 10251</code>), and <code>kube-controller-manager</code> (<code>TCP 10257</code>). When polling a Kubernetes component's metrics endpoint, metrics from other components are not filtered out.</p> <p>A k3s master running all Kubernetes components, emits the same metrics on all ports. On the other hand, k3s workers running only <code>kubelet</code> and <code>kube-proxy</code>, emit the same metrics on <code>TCP 10250</code> and <code>10249</code> ports.</p> <p>Enabling scraping of all different metrics TCP ports (<code>10249</code>, <code>10250</code>, <code>10251</code>, <code>10257</code>, and <code>apiserver</code>) causes ingestion of duplicate metrics. To reduce memory and CPU consumption, Prometheus should avoid duplicate metrics. On the other hand, kubelet's additional metrics endpoints (<code>/metrics/cadvisor</code>, <code>/metrics/resource</code>, and <code>/metrics/probes</code>) are only available at <code>TCP 10250</code>.</p> <p>Therefore, the solution is to scrape only the metrics endpoints available on the kubelet port (<code>TCP 10250</code>): <code>/metrics</code>, <code>/metrics/cadvisor</code>, <code>/metrics/resource</code>, and <code>/metrics/probes</code>.</p> <p>To avoid duplicates, reduce memory consumption and minimize costs the following configuration are applied:</p> <ul> <li> <p><code>apiserver</code> (<code>{job=\"apiserver\"}</code>)</p> </li> <li> <p><code>kube-controller-manager</code> (<code>{job=\"kube-controller-manager\"}</code>)</p> </li> <li> <p><code>kube-scheduler</code> (<code>{job=\"kube-scheduler\"}</code>)</p> </li> <li> <p><code>kubelet</code> (<code>{job=\"kubelet\"}</code>)</p> </li> <li> <p><code>kube-state-metrics</code> (<code>{job=\"kube-state-metrics\"}</code>)</p> </li> <li> <p><code>kube-proxy</code> (<code>{job=\"kube-proxy\"}</code>)</p> </li> </ul>"},{"location":"archive/metrics/prometheus/#monitoring","title":"Monitoring","text":""},{"location":"archive/metrics/prometheus/#alerting","title":"Alerting","text":""},{"location":"archive/metrics/prometheus/#references","title":"References","text":"<ul> <li> <p>Prevent metrics explosion</p> </li> <li> <p>kube-prometheus-runbooks</p> </li> <li> <p>Amazon Managed Prometheus</p> </li> <li> <p>Kubernetes API server metrics</p> </li> <li> <p>Kubernetes Metrics reference</p> </li> <li> <p>PromQL Queries for Exploring Your Metrics</p> </li> <li> <p>My Prometheus is overwhelmed</p> </li> </ul>"}]}